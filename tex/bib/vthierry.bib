@inproceedings{bardenet2013collaborative,
  title={Collaborative hyperparameter tuning},
  author={Bardenet, R{\'e}mi and Brendel, M{\'a}ty{\'a}s and K{\'e}gl, Bal{\'a}zs and Sebag, Michele},
  booktitle={International Conference on Machine Learning},
  pages={199--207},
  year={2013}
}

@inproceedings{bergstra2013hyperopt,
  title={Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms},
  author={Bergstra, James and Yamins, Dan and Cox, David D},
  booktitle={Proceedings of the 12th Python in Science Conference},
  pages={13--20},
  year={2013},
  organization={Citeseer}
}

@article{zimek2012survey,
  title={A survey on unsupervised outlier detection in high-dimensional numerical data},
  author={Zimek, Arthur and Schubert, Erich and Kriegel, Hans-Peter},
  journal={Statistical Analysis and Data Mining: The ASA Data Science Journal},
  volume={5},
  number={5},
  pages={363--387},
  year={2012},
  publisher={Wiley Online Library}
}

@article{ghodsi2006dimensionality,
  title={Dimensionality reduction a short tutorial},
  author={Ghodsi, Ali},
  journal={Department of Statistics and Actuarial Science, Univ. of Waterloo, Ontario, Canada},
  volume={37},
  pages={38},
  year={2006}
}

@article{trunk1979problem,
  title={A problem of dimensionality: A simple example},
  author={Trunk, Gerard V},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  number={3},
  pages={306--307},
  year={1979},
  publisher={IEEE}
}

@article{van2009dimensionality,
  title={Dimensionality reduction: a comparative review},
  author={Van Der Maaten, Laurens and Postma, Eric and Van den Herik, Jaap},
  journal={J Mach Learn Res},
  volume={10},
  pages={66--71},
  year={2009}
}

@article{perraultmetric,
  title={Metric Learning of Manifolds},
  author={Perrault-Joncas, Dominique and Meila, Marina}
}

@article{gudmundsson2008introduction,
  title={An introduction to Riemannian geometry},
  author={Gudmundsson, Sigmundur},
  journal={Lund University},
  year={2008}
}

@incollection{lee2003smooth,
  title={Smooth manifolds},
  author={Lee, John M},
  booktitle={Introduction to Smooth Manifolds},
  pages={1--29},
  year={2003},
  publisher={Springer}
}

@inproceedings{hautamaki2004outlier,
  title={Outlier detection using k-nearest neighbour graph},
  author={Hautamaki, Ville and Karkkainen, Ismo and Franti, Pasi},
  booktitle={Pattern Recognition, 2004. ICPR 2004. Proceedings of the 17th International Conference on},
  volume={3},
  pages={430--433},
  year={2004},
  organization={IEEE}
}

@article{rohini2017outlier,
  title={Outlier Detection on High Dimensional Data Using RNN},
  author={Rohini, R and Pepsi, M Blessa Binolin},
  journal={European Journal of Applied Sciences},
  volume={9},
  number={1},
  pages={44--49},
  year={2017}
}

@inproceedings{singh2003high,
  title={High dimensional reverse nearest neighbor queries},
  author={Singh, Amit and Ferhatosmanoglu, Hakan and Tosun, Ali {\c{S}}aman},
  booktitle={Proceedings of the twelfth international conference on Information and knowledge management},
  pages={91--98},
  year={2003},
  organization={ACM}
}

@article{osher2017low,
  title={Low dimensional manifold model for image processing},
  author={Osher, Stanley and Shi, Zuoqiang and Zhu, Wei},
  journal={SIAM Journal on Imaging Sciences},
  volume={10},
  number={4},
  pages={1669--1690},
  year={2017},
  publisher={SIAM}
}

@article{belkin2003laplacian,
  title={Laplacian eigenmaps for dimensionality reduction and data representation},
  author={Belkin, Mikhail and Niyogi, Partha},
  journal={Neural computation},
  volume={15},
  number={6},
  pages={1373--1396},
  year={2003},
  publisher={MIT Press}
}

@article{boyd2011distributed,
  title={Distributed optimization and statistical learning via the alternating direction method of multipliers},
  author={Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan and others},
  journal={Foundations and Trends{\textregistered} in Machine learning},
  volume={3},
  number={1},
  pages={1--122},
  year={2011},
  publisher={Now Publishers, Inc.}
}

@article{friedman1997bias,
  title={On bias, variance, 0/1—loss, and the curse-of-dimensionality},
  author={Friedman, Jerome H},
  journal={Data mining and knowledge discovery},
  volume={1},
  number={1},
  pages={55--77},
  year={1997},
  publisher={Springer}
}

@article{lewis2002globally,
  title={A globally convergent augmented Lagrangian pattern search algorithm for optimization with general constraints and simple bounds},
  author={Lewis, Robert Michael and Torczon, Virginia},
  journal={SIAM Journal on Optimization},
  volume={12},
  number={4},
  pages={1075--1089},
  year={2002},
  publisher={SIAM}
}
@inproceedings{indyk1998approximate,
  title={Approximate nearest neighbors: towards removing the curse of dimensionality},
  author={Indyk, Piotr and Motwani, Rajeev},
  booktitle={Proceedings of the thirtieth annual ACM symposium on Theory of computing},
  pages={604--613},
  year={1998},
  organization={ACM}
}

@article{luo2007convergence,
  title={On the convergence of augmented Lagrangian methods for constrained global optimization},
  author={Luo, HZ and Sun, XL and Li, Duan},
  journal={SIAM Journal on Optimization},
  volume={18},
  number={4},
  pages={1209--1230},
  year={2007},
  publisher={SIAM}
}

@inproceedings{haralick2005linear,
  title={Linear manifold clustering},
  author={Haralick, Robert and Harpaz, Rave},
  booktitle={International Workshop on Machine Learning and Data Mining in Pattern Recognition},
  pages={132--141},
  year={2005},
  organization={Springer}
}

@article{haralick2007linear,
  title={Linear manifold clustering in high dimensional spaces by stochastic search},
  author={Haralick, Robert and Harpaz, Rave},
  journal={Pattern recognition},
  volume={40},
  number={10},
  pages={2672--2684},
  year={2007},
  publisher={Elsevier}
}

@article{huang2003unified,
  title={A unified augmented Lagrangian approach to duality and exact penalization},
  author={Huang, XX and Yang, XQ},
  journal={Mathematics of Operations Research},
  volume={28},
  number={3},
  pages={533--552},
  year={2003},
  publisher={INFORMS}
}

@article{zhu2018scientific,
  title={Scientific data interpolation with low dimensional manifold model},
  author={Zhu, Wei and Wang, Bao and Barnard, Richard and Hauck, Cory D and Jenko, Frank and Osher, Stanley},
  journal={Journal of Computational Physics},
  volume={352},
  pages={213--245},
  year={2018},
  publisher={Elsevier}
}

@article{li2017robust,
  title={Robust sparse estimation tasks in high dimensions},
  author={Li, Jerry},
  journal={arXiv preprint arXiv:1702.05860},
  year={2017}
}

@article{li2017point,
  title={Point integral method for solving poisson-type equations on manifolds from point clouds with convergence guarantees},
  author={Li, Zhen and Shi, Zuoqiang and Sun, Jian},
  journal={Communications in Computational Physics},
  volume={22},
  number={1},
  pages={228--258},
  year={2017},
  publisher={Cambridge University Press}
}

@article{mukherjee2010learning,
  title={Learning gradients on manifolds},
  author={Mukherjee, Sayan and Wu, Qiang and Zhou, Ding-Xuan},
  journal={Bernoulli},
  pages={181--207},
  year={2010},
  publisher={JSTOR}
}

@inproceedings{aggarwal2001surprising,
  title={On the surprising behavior of distance metrics in high dimensional space},
  author={Aggarwal, Charu C and Hinneburg, Alexander and Keim, Daniel A},
  booktitle={International conference on database theory},
  pages={420--434},
  year={2001},
  organization={Springer}
}

@article{pandove2017local,
  title={Local graph based correlation clustering},
  author={Pandove, Divya and Rani, Rinkle and Goel, Shivani},
  journal={Knowledge-Based Systems},
  volume={138},
  pages={155--175},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{yogatama2014efficient,
  title={Efficient transfer learning method for automatic hyperparameter tuning},
  author={Yogatama, Dani and Mann, Gideon},
  booktitle={Artificial Intelligence and Statistics},
  pages={1077--1085},
  year={2014}
}

@inproceedings{pedregosa2016hyperparameter,
  title={Hyperparameter optimization with approximate gradient},
  author={Pedregosa, Fabian},
  booktitle={International conference on machine learning},
  pages={737--746},
  year={2016}
}

@inproceedings{maclaurin2015gradient,
  title={Gradient-based hyperparameter optimization through reversible learning},
  author={Maclaurin, Dougal and Duvenaud, David and Adams, Ryan},
  booktitle={International Conference on Machine Learning},
  pages={2113--2122},
  year={2015}
}

@article{luo2016review,
  title={A review of automatic selection methods for machine learning algorithms and hyper-parameter values},
  author={Luo, Gang},
  journal={Network Modeling Analysis in Health Informatics and Bioinformatics},
  volume={5},
  number={1},
  pages={18},
  year={2016},
  publisher={Springer}
}

@inproceedings{kusner2015differentially,
  title={Differentially private Bayesian optimization},
  author={Kusner, Matt and Gardner, Jacob and Garnett, Roman and Weinberger, Kilian},
  booktitle={International Conference on Machine Learning},
  pages={918--927},
  year={2015}
}

@inproceedings{jamieson2016non,
  title={Non-stochastic best arm identification and hyperparameter optimization},
  author={Jamieson, Kevin and Talwalkar, Ameet},
  booktitle={Artificial Intelligence and Statistics},
  pages={240--248},
  year={2016}
}

@article{tsamardinos2015performance,
  title={Performance-estimation properties of cross-validation-based protocols with simultaneous hyper-parameter optimization},
  author={Tsamardinos, Ioannis and Rakhshani, Amin and Lagani, Vincenzo},
  journal={International Journal on Artificial Intelligence Tools},
  volume={24},
  number={05},
  pages={1540023},
  year={2015},
  publisher={World Scientific}
}

@phdthesis{bermudez2014automatic,
  title={Automatic problem-specific hyperparameter optimization and model selection for supervised machine learning},
  author={Berm{\'u}dez Chac{\'o}n, R{\'o}ger},
  year={2014}
}

@inproceedings{zheng2013lazy,
  title={Lazy Paired Hyper-Parameter Tuning.},
  author={Zheng, Alice X and Bilenko, Mikhail},
  booktitle={IJCAI},
  pages={1924--1931},
  year={2013}
}

@inproceedings{snoek2012practical,
  title={Practical bayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  booktitle={Advances in neural information processing systems},
  pages={2951--2959},
  year={2012}
}

@inproceedings{bergstra2011algorithms,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James S and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  booktitle={Advances in neural information processing systems},
  pages={2546--2554},
  year={2011}
}

@article{pham2018efficient,
  title={Efficient Neural Architecture Search via Parameter Sharing},
  author={Pham, Hieu and Guan, Melody Y and Zoph, Barret and Le, Quoc V and Dean, Jeff},
  journal={arXiv preprint arXiv:1802.03268},
  year={2018}
}

@inproceedings{qin2017improving,
  title={Improving the Expected Improvement Algorithm},
  author={Qin, Chao and Klabjan, Diego and Russo, Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5387--5397},
  year={2017}
}

@article{cortes2016adanet,
  title={Adanet: Adaptive structural learning of artificial neural networks},
  author={Cortes, Corinna and Gonzalvo, Xavi and Kuznetsov, Vitaly and Mohri, Mehryar and Yang, Scott},
  journal={arXiv preprint arXiv:1607.01097},
  year={2016}
}

@article{marceau2016practical,
  title={Practical Riemannian neural networks},
  author={Marceau-Caron, Ga{\'e}tan and Ollivier, Yann},
  journal={arXiv preprint arXiv:1602.08007},
  year={2016}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{li2016efficient,
  title={Efficient hyperparameter optimization and infinitely many armed bandits},
  author={Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  journal={CoRR, abs/1603.06560},
  year={2016}
}

@article{kotthoff2017auto,
  title={Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA},
  author={Kotthoff, Lars and Thornton, Chris and Hoos, Holger H and Hutter, Frank and Leyton-Brown, Kevin},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={826--830},
  year={2017},
  publisher={JMLR. org}
}

@article{burke2013hyper,
  title={Hyper-heuristics: A survey of the state of the art},
  author={Burke, Edmund K and Gendreau, Michel and Hyde, Matthew and Kendall, Graham and Ochoa, Gabriela and {\"O}zcan, Ender and Qu, Rong},
  journal={Journal of the Operational Research Society},
  volume={64},
  number={12},
  pages={1695--1724},
  year={2013},
  publisher={Springer}
}

@article{wolpert1997no,
  title={No free lunch theorems for optimization},
  author={Wolpert, David H and Macready, William G},
  journal={IEEE transactions on evolutionary computation},
  volume={1},
  number={1},
  pages={67--82},
  year={1997},
  publisher={IEEE}
}

@article{belkin2006manifold,
  title={Manifold regularization: A geometric framework for learning from labeled and unlabeled examples},
  author={Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
  journal={Journal of machine learning research},
  volume={7},
  number={Nov},
  pages={2399--2434},
  year={2006}
}

@phdthesis{zhu2017nonlocal,
  title={Nonlocal Variational Methods in Image and Data Processing},
  author={Zhu, Wei},
  year={2017},
  school={University of California, Los Angeles}
}

@article{zhu2017ldmnet,
  title={LDMNet: Low Dimensional Manifold Regularized Neural Networks},
  author={Zhu, Wei and Qiu, Qiang and Huang, Jiaji and Calderbank, Robert and Sapiro, Guillermo and Daubechies, Ingrid},
  journal={arXiv preprint arXiv:1711.06246},
  year={2017}
}

@article{huynh2009metrics,
  title={Metrics for 3D rotations: Comparison and analysis},
  author={Huynh, Du Q},
  journal={Journal of Mathematical Imaging and Vision},
  volume={35},
  number={2},
  pages={155--164},
  year={2009},
  publisher={Springer}
}

@article{vieville:inria-00000172,
  TITLE = {{Implementing a multi-model estimation method}},
  AUTHOR = {Vi{\'e}ville, Thierry and Lingrand, Diane and Gaspard, Fran{\c c}ois},
  URL = {https://hal.inria.fr/inria-00000172},
  JOURNAL = {{International Journal of Computer Vision}},
  PUBLISHER = {{Springer Verlag}},
  YEAR = {2001},
  MONTH = Oct,
  PDF = {https://hal.inria.fr/inria-00000172/file/vieville-lingrand-etal_01.pdf},
  HAL_ID = {inria-00000172},
  HAL_VERSION = {v1},
}

@techreport{vieville:hal-01610735,
  TITLE = {{Recurrent neural network weight estimation though backward tuning}},
  AUTHOR = {Vi{\'e}ville, Thierry and Hinaut, Xavier and Drumond, Thalita F and Alexandre, Fr{\'e}d{\'e}ric},
  URL = {https://hal.inria.fr/hal-01610735},
  TYPE = {Research Report},
  NUMBER = {RR-9100},
  PAGES = {1-54},
  INSTITUTION = {{Inria Bordeaux Sud-Ouest}},
  YEAR = {2017},
  MONTH = Oct,
  KEYWORDS = {recurrent network ; backward tuning ; machine learning ; apprentissage automatique ; r{\'e}seaux r{\'e}currents ; ajustement r{\'e}troactif},
  PDF = {https://hal.inria.fr/hal-01610735/file/RR-9100.pdf},
  HAL_ID = {hal-01610735},
  HAL_VERSION = {v1},
}

@book{collard2007reasoning,
  title={Reasoning about program transformations: imperative programming and flow of data},
  author={Collard, Jean-Francois},
  year={2007},
  publisher={Springer Science \& Business Media}
}

@article{conrad2004probability,
  title={Probability distributions and maximum entropy},
  author={Conrad, Keith},
  journal={Entropy},
  volume={6},
  number={452},
  pages={10},
  year={2004}
}

@techreport{vieville:inria-00074888,
  TITLE = {{Using pseudo Kalman-filters in the presence of constraints application to sensing behaviors}},
  AUTHOR = {Vi{\'e}ville, Thierry and Sander, Peter},
  URL = {https://hal.inria.fr/inria-00074888},
  TYPE = {Research Report},
  NUMBER = {RR-1669},
  INSTITUTION = {{INRIA}},
  YEAR = {1992},
  PDF = {https://hal.inria.fr/inria-00074888/file/RR-1669.pdf},
  HAL_ID = {inria-00074888},
  HAL_VERSION = {v1},
}

@article{angluin1983inductive,
 author = {Angluin, Dana and Smith, Carl H.},
 title = {Inductive Inference: Theory and Methods},
 journal = {ACM Comput. Surv.},
 issue_date = {Sept. 1983},
 volume = {15},
 number = {3},
 month = sep,
 year = {1983},
 issn = {0360-0300},
 pages = {237--269},
 numpages = {33},
 url = {http://doi.acm.org/10.1145/356914.356918},
 doi = {10.1145/356914.356918},
 acmid = {356918},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
@article{angluin2004queries,
  title={Queries revisited},
  author={Angluin, Dana},
  journal={Theoretical Computer Science},
  volume={313},
  number={2},
  pages={175--194},
  year={2004},
  publisher={Elsevier}
}
@article{mouret2016micro,
  title={Micro-Data Learning: The Other End of the Spectrum},
  author={Mouret, Jean-Baptiste},
  journal={arXiv preprint arXiv:1610.00946},
  year={2016}
}
@article{Cucker2002Mathematical,
    author = {Cucker, Felipe and Smale, Steve},
    doi = {10.1090/S0273-0979-01-00923-5},
    isbn = {0884-8289},
    issn = {02730979},
    journal = {Bulletin of the American Mathematical Society},
    number = {1},
    pages = {1--49},
    pmid = {18382608},
    title = {On the mathematical foundations of learning},
    volume = {39},
    year = {2002}
}

@misc{Ruder2017Transfer,
    author = {Sebastien Ruder},
    pages = {1--36},
    title = {Transfer Learning - Machine Learning ' s Next Frontier},
    year = {2017}
}

@book{popper2005logic,
  title={The logic of scientific discovery},
  author={Popper, Karl},
  year={2005},
  publisher={Routledge}
}
@article{haddad1981monotone,
  title={Monotone viable trajectories for functional differential inclusions},
  author={Haddad, Georges},
  journal={Journal of Differential Equations},
  volume={42},
  number={1},
  pages={1--24},
  year={1981},
  publisher={Elsevier}
}
@article{schwartenbeck2013exploration,
  title={Exploration, novelty, surprise, and free energy minimization},
  author={Schwartenbeck, Philipp and FitzGerald, Thomas and Dolan, Raymond J and Friston, Karl},
  journal={Frontiers in psychology},
  volume={4},
  year={2013},
  publisher={Frontiers Media SA}
}
@article{friston2016active1,
  title={Active inference: A process theory},
  author={Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and Pezzulo, Giovanni},
  journal={Neural computation},
  year={2016},
  publisher={MIT Press}
}
@article{friston2016active2,
  title={Active inference and learning},
  author={Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and Pezzulo, Giovanni and others},
  journal={Neuroscience \& Biobehavioral Reviews},
  volume={68},
  pages={862--879},
  year={2016},
  publisher={Elsevier}
}
@book{valiant2013probably,
  title={Probably Approximately Correct: Nature{\~O}s Algorithms for Learning and Prospering in a Complex World},
  author={Valiant, Leslie},
  year={2013},
  publisher={Basic Books (AZ)}
}
@article{marr1976understanding,
  title={From understanding computation to understanding neural circuitry},
  author={Marr, David and Poggio, Tomaso},
  year={1976}
}
@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  volume={1},
  number={1},
  year={1998},
  publisher={MIT press Cambridge}
}
@article{windridge2017emergent,
  title={Emergent intentionality in perception-action subsumption hierarchies},
  author={Windridge, David},
  journal={Frontiers in Robotics and AI},
  volume={4},
  pages={38},
  year={2017},
  publisher={Frontiers}
}
@article{redish2007reconciling,
  title={Reconciling reinforcement learning models with behavioral extinction and renewal: implications for addiction, relapse, and problem gambling.},
  author={Redish, A David and Jensen, Steve and Johnson, Adam and Kurth-Nelson, Zeb},
  journal={Psychological review},
  volume={114},
  number={3},
  pages={784},
  year={2007},
  publisher={American Psychological Association}
}
@article{cardinal2002emotion,
  title={Emotion and motivation: the role of the amygdala, ventral striatum, and prefrontal cortex},
  author={Cardinal, Rudolf N and Parkinson, John A and Hall, Jeremy and Everitt, Barry J},
  journal={Neuroscience \& Biobehavioral Reviews},
  volume={26},
  number={3},
  pages={321--352},
  year={2002},
  publisher={Elsevier}
}
@article{balleine2006parallel,
  title={Parallel incentive processing: an integrated view of amygdala function},
  author={Balleine, Bernard W and Killcross, Simon},
  journal={Trends in neurosciences},
  volume={29},
  number={5},
  pages={272--279},
  year={2006},
  publisher={Elsevier}
}
@article{koechlin2007information,
  title={An information theoretical approach to prefrontal executive function},
  author={Koechlin, Etienne and Summerfield, Christopher},
  journal={Trends in cognitive sciences},
  volume={11},
  number={6},
  pages={229--235},
  year={2007},
  publisher={Elsevier}
}

@article{gershman2010context,
  title={Context, learning, and extinction.},
  author={Gershman, Samuel J and Blei, David M and Niv, Yael},
  journal={Psychological review},
  volume={117},
  number={1},
  pages={197},
  year={2010},
  publisher={American Psychological Association}
}

@article{carrere:hal-01145790,
  TITLE = {{A pavlovian model of the amygdala and its influence within the medial temporal lobe}},
  AUTHOR = {Carrere, Maxime and Alexandre, Fr{\'e}d{\'e}ric},
  URL = {https://hal.inria.fr/hal-01145790},
  JOURNAL = {{Frontiers in Systems Neuroscience}},
  PUBLISHER = {{Frontiers}},
  PAGES = {14},
  YEAR = {2015},
  MONTH = Mar,
  DOI = {10.3389/fnsys.2015.00041},
  KEYWORDS = {hippocampus ; acetylcholine ; pavlovian conditioning ; infralimbic cortex ; perirhinal cortex ; amygdala},
  PDF = {https://hal.inria.fr/hal-01145790/file/fnsys-09-00041.pdf},
  HAL_ID = {hal-01145790},
  HAL_VERSION = {v1},
}

@inproceedings{OReilly2014GoalDrivenCI,
  title={Goal-Driven Cognition in the Brain: A Computational Framework},
  author={Randall C. O'Reilly and Thomas E. Hazy and Jessica Mollick and Prescott Mackie and Seth Herd},
  year={2014}
}

@unpublished{alexandre:hal-01246653,
  TITLE = {{A behavioral framework for a systemic view of brain modeling}},
  AUTHOR = {Alexandre, Fr{\'e}d{\'e}ric},
  URL = {https://hal.inria.fr/hal-01246653},
  NOTE = {working paper or preprint},
  YEAR = {2015},
  MONTH = Dec,
  PDF = {https://hal.inria.fr/hal-01246653/file/ChapterAlexandre.pdf},
  HAL_ID = {hal-01246653},
  HAL_VERSION = {v1},
}


@Article{Poggio2017,
author="Poggio, Tomaso
and Mhaskar, Hrushikesh
and Rosasco, Lorenzo
and Miranda, Brando
and Liao, Qianli",
title="Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review",
journal="International Journal of Automation and Computing",
year="2017",
month="Mar",
day="14",
abstract="The paper reviews and extends an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. A class of deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Implications of a few key theorems are discussed, together with new results, open problems and conjectures.",
issn="1751-8520",
doi="10.1007/s11633-017-1054-2",
url="https://doi.org/10.1007/s11633-017-1054-2"
}


@article{HUANG2006489,
title = "Extreme learning machine: Theory and applications",
journal = "Neurocomputing",
volume = "70",
number = "1",
pages = "489 - 501",
year = "2006",
note = "Neural Networks",
issn = "0925-2312",
doi = "http://dx.doi.org/10.1016/j.neucom.2005.12.126",
url = "http://www.sciencedirect.com/science/article/pii/S0925231206000385",
author = "Guang-Bin Huang and Qin-Yu Zhu and Chee-Kheong Siew",
keywords = "Feedforward neural networks",
keywords = "Back-propagation algorithm",
keywords = "Extreme learning machine",
keywords = "Support vector machine",
keywords = "Real-time learning",
keywords = "Random node"
}

@inproceedings{He2016Deep,
    archiveprefix = {arXiv},
    arxivid = {1512.03385},
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    eprint = {1512.03385},
    link = {http://arxiv.org/pdf/1512.03385v1.pdf},
    pages = {770--778},
    title = {Deep Residual Learning for Image Recognition},
    year = {2016}
}

@article{Zeiler2014Visualizing,
    archiveprefix = {arXiv},
    arxivid = {1311.2901},
    author = {Zeiler, Matthew D. and Fergus, Rob},
    doi = {10.1007/978-3-319-10590-1_53},
    eprint = {1311.2901},
    isbn = {978-3-319-10589-5},
    issn = {978-3-319-10589-5},
    journal = {Lecture Notes in Computer Science},
    link = {http://arxiv.org/abs/1311.2901 http://link.springer.com/10.1007/978-3-319-10590-1_53},
    pages = {818--833},
    pmid = {26353135},
    title = {Visualizing and Understanding Convolutional Networks},
    volume = {8689},
    year = {2014}
}

@article{Szegedy2014Going,
    archiveprefix = {arXiv},
    arxivid = {1409.4842},
    author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
    eprint = {1409.4842},
    link = {http://arxiv.org/abs/1409.4842},
    month = {sep},
    title = {Going Deeper with Convolutions},
    year = {2014}
}

@article{Szegedy2016Inception,
    archiveprefix = {arXiv},
    arxivid = {1602.07261},
    author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
    eprint = {1602.07261},
    journal = {Arxiv},
    link = {http://arxiv.org/abs/1602.07261},
    month = {feb},
    pages = {12},
    title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
    year = {2016}
}

@inproceedings{Simonyan2015Very,
    archiveprefix = {arXiv},
    arxivid = {1409.1556},
    author = {Simonyan, Karen and Zisserman, Andrew},
    booktitle = {International Conference on Learning Representations (ICRL)},
    eprint = {1409.1556},
    link = {http://arxiv.org/abs/1409.1556},
    month = {sep},
    pages = {1--14},
    title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
    year = {2015}
}

@article{Sermanet2013Overfeat,
    archiveprefix = {arXiv},
    arxivid = {1312.6229},
    author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
    eprint = {1312.6229},
    link = {http://arxiv.org/abs/1312.6229},
    month = {dec},
    title = {OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks},
    year = {2013}
}

@inproceedings{Krizhevsky2012Imagenet,
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
    booktitle = {Advances in Neural Information Processing Systems 25},
    editor = {Weinberger, F. Pereira and C. J. C. Burges and L. Bottou and K. Q.},
    pages = {1097--1105},
    publisher = {Curran Associates, Inc.},
    title = {ImageNet Classification with Deep Convolutional Neural Networks},
    year = {2012}
}

@article{Lecun1998Gradient,
    author = {Lecun, Y. and Bottou, L and Bengio, Y and Haffner, P},
    doi = {10.1109/5.726791},
    issn = {00189219},
    journal = {Proceedings of the IEEE},
    link = {http://ieeexplore.ieee.org/document/726791/},
    number = {11},
    pages = {2278--2324},
    title = {Gradient-based learning applied to document recognition},
    volume = {86},
    year = {1998}
}

@inproceedings{Drumond2017From,
author = {F. Drumond, Thalita and Vi\'eville, Thierry and Alexandre, Frédéric},
title = {Using prototypes to improve convolutional networks interpretability},
booktitle = {31st Annual Conference on Neural Information Processing Systems: Transparent and interpretable machine learning in safety critical environments Workshop},
year = {2017}
}

@article{topalidou_long_2015,
	title = {A long journey into reproducible computational neuroscience},
	volume = {9},
	issn = {1662-5188},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4350388/},
	doi = {10.3389/fncom.2015.00030},
	journal = {Frontiers in Computational Neuroscience},
	author = {Topalidou, Meropi and Leblois, Arthur and Boraud, Thomas and Rougier, Nicolas P},
	year = {2015},
	pages = {30}
}

@article{Zhang2016Understanding,
    archiveprefix = {arXiv},
    arxivid = {1611.03530},
    author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
    eprint = {1611.03530},
    link = {http://arxiv.org/abs/1611.03530},
    month = {nov},
    title = {Understanding deep learning requires rethinking generalization},
    year = {2016}
}


@article{Fdrumond2017,
   author = {F. Drumond, Thalita and Vi\'eville, Thierry and Alexandre, Fr\'ed\'eric Alexandre},
    title = {Not-so-big data deep learning: a review},
    note ={in preparation},
    year = {2017}
}

@article{Bengio2012Representation,
    archiveprefix = {arXiv},
    arxivid = {1206.5538},
    author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
    doi = {10.1109/TPAMI.2013.50},
    eprint = {1206.5538},
    isbn = {0162-8828 VO - 35},
    issn = {1939-3539},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    link = {http://www.ncbi.nlm.nih.gov/pubmed/23459267},
    number = {8},
    pages = {1798--1828},
    pmid = {23787338},
    title = {Representation Learning: A Review and New Perspectives},
    volume = {35},
    year = {2012}
}

@article{Bengio2016Towards,
    archiveprefix = {arXiv},
    arxivid = {1502.04156},
    author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Lin, Zhouhan},
    doi = {10.1007/s13398-014-0173-7.2},
    eprint = {1502.04156},
    isbn = {9781467398947},
    issn = {0717-6163},
    journal = {arXiv preprint arxiv:1502.0415},
    link = {http://arxiv.org/abs/1502.04156},
    month = {feb},
    pages = {10},
    pmid = {15003161},
    title = {Towards Biologically Plausible Deep Learning},
    year = {2016}
}
@article{Freund2003Efficient,
    author = {Freund, Yoav and Iyer, Raj and Schapire, Robert E and Singer, Yoram},
    journal = {The Journal of machine learning research},
    pages = {933--969},
    publisher = {JMLR. org},
    title = {An efficient boosting algorithm for combining preferences},
    volume = {4},
    year = {2003}
}

@article{Cho-2014,
  author    = {Kyunghyun Cho and
               Bart van Merrienboer and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Fethi Bougares and
               Holger Schwenk and
               Yoshua Bengio},
  title     = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical
               Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1406.1078},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.1078},
  timestamp = {Wed, 02 Jul 2014 08:24:58 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChoMGBSB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Legenstein:2007,
title = "Edge of chaos and prediction of computational performance for neural circuit models ",
journal = "Neural Networks ",
volume = "20",
number = "3",
pages = "323 - 334",
year = "2007",
note = "Echo State Networks and Liquid State Machines ",
issn = "0893-6080",
doi = "http://dx.doi.org/10.1016/j.neunet.2007.04.017",
url = "http://www.sciencedirect.com/science/article/pii/S0893608007000433",
author = "Robert Legenstein and Wolfgang Maass",
keywords = "Neural networks",
keywords = "Spiking networks",
keywords = "Edge of chaos",
keywords = "Microcircuits",
keywords = "Computational performance",
keywords = "Network dynamics ",
abstract = "We analyze in this article the significance of the edge of chaos for real-time computations in neural microcircuit models consisting of spiking neurons and dynamic synapses. We find that the edge of chaos predicts quite well those values of circuit parameters that yield maximal computational performance. But obviously it makes no prediction of their computational performance for other parameter values. Therefore, we propose a new method for predicting the computational performance of neural microcircuit models. The new measure estimates directly the kernel property and the generalization capability of a neural microcircuit. We validate the proposed measure by comparing its prediction with direct evaluations of the computational performance of various neural microcircuit models. The proposed method also allows us to quantify differences in the computational performance and generalization capability of neural circuits in different dynamic regimes (UP- and DOWN-states) that have been demonstrated through intracellular recordings in vivo. "
}
@book{Goodfellow2016Deep,
    author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
    link = {http://www.deeplearningbook.org},
    publisher = {MIT Press},
    title = {Deep Learning},
    year = {2016}
}
@book{Nielsen2015,
    author = {Nielsen, Michael},
    link = {http://www.deeplearningbook.org},
    publisher = {Determination Press},
    title = {Neural Networks and Deep Learning},
    year = {2015}
}

@article{Acevedo-Mosqueda:2013:BAM:2431211.2431217,
 author = {Acevedo-Mosqueda, Maria Elena and Y\'{a}\~{n}ez-M\'{a}rquez, Cornelio and Acevedo-Mosqueda, Marco Antonio},
 title = {Bidirectional Associative Memories: Different Approaches},
 journal = {ACM Comput. Surv.},
 issue_date = {February 2013},
 volume = {45},
 number = {2},
 month = mar,
 year = {2013},
 issn = {0360-0300},
 pages = {18:1--18:30},
 articleno = {18},
 numpages = {30},
 url = {http://doi.acm.org/10.1145/2431211.2431217},
 doi = {10.1145/2431211.2431217},
 acmid = {2431217},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Bidirectional associative memories, correct recall, one-shot algorithms},
} 




@article{doi:10.1162/089976603762552988,
author = { Xiaohui Xie  and  H. Sebastian Seung },
title = {Equivalence of Backpropagation and Contrastive Hebbian Learning in a Layered Network},
journal = {Neural Computation},
volume = {15},
number = {2},
pages = {441-454},
year = {2003},
doi = {10.1162/089976603762552988},

URL = { 
        https://doi.org/10.1162/089976603762552988
    
},
eprint = { 
        https://doi.org/10.1162/089976603762552988
    
}
,
    abstract = { Backpropagation and contrastive Hebbian learning are two methods of training networks with hidden neurons. Backpropagation computes an error signal for the output neurons and spreads it over the hidden neurons. Contrastive Hebbian learning involves clamping the output neurons at desired values and letting the effect spread through feedback connections over the entire network. To investigate the relationship between these two forms of learning, we consider a special case in which they are identical: a multilayer perceptron with linear output units, to which weak feedback connections have been added. In this case, the change in network state caused by clamping the output neurons turns out to be the same as the error signal spread by backpropagation, except for a scalar prefactor. This suggests that the functionality of backpropagation can be realized alternatively by a Hebbian-type learning algorithm, which is suitable for implementation in biological networks. }
}




@article{Schmidhuber:2015,
author = "J. Schmidhuber",
title = "Deep Learning in Neural Networks: An Overview",
journal = "Neural Networks",
pages = "85-117",
volume = "61",
doi = "10.1016/j.neunet.2014.09.003",
note = "Published online 2014; based on TR arXiv:1404.7828 [cs.NE]",
year = "2015"} 

@inproceedings{He2016Deep,
    archiveprefix = {arXiv},
    arxivid = {1512.03385},
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    eprint = {1512.03385},
    link = {http://arxiv.org/pdf/1512.03385v1.pdf},
    pages = {770--778},
    title = {Deep Residual Learning for Image Recognition},
    year = {2016}
}


@article{Balduzzi:2016,
  author    = {David Balduzzi and
               Muhammad Ghifary},
  title     = {Strongly-Typed Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1602.02218},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.02218},
  timestamp = {Tue, 01 Mar 2016 17:47:25 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BalduzziG16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Gers:2003,
 author = {Gers, Felix A. and Schraudolph, Nicol N. and Schmidhuber, J\"{u}rgen},
 title = {Learning Precise Timing with LSTM Recurrent Networks},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2003},
 volume = {3},
 month = mar,
 year = {2003},
 issn = {1532-4435},
 pages = {115--143},
 numpages = {29},
 url = {http://dx.doi.org/10.1162/153244303768966139},
 doi = {10.1162/153244303768966139},
 acmid = {944925},
 publisher = {JMLR.org},
 keywords = {long short-term memory, recurrent neural networks, timing},
} 

@article{Hochreiter:1997,
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 title = {Long Short-Term Memory},
 journal = {Neural Comput.},
 issue_date = {November 15, 1997},
 volume = {9},
 number = {8},
 month = nov,
 year = {1997},
 issn = {0899-7667},
 pages = {1735--1780},
 numpages = {46},
 url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
 doi = {10.1162/neco.1997.9.8.1735},
 acmid = {1246450},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@incollection{beurle_storage_1962,
	title = {Storage and {Manipulation} of {Information} in {Random} {Networks}},
	copyright = {©1962 Springer Science+Business Media New York},
	isbn = {978-1-4899-6269-0 978-1-4899-6584-4},
	url = {http://link.springer.com.gate1.inist.fr/chapter/10.1007/978-1-4899-6584-4_3},
	abstract = {Innumerable questions can be asked in relation to the brain. How does it work? How should a “thinking machine” work? How is it that there is in the universe the consistency which makes a “thinking machine” possible?},
	language = {en},
	urldate = {2014-10-31},
	booktitle = {Aspects of the {Theory} of {Artificial} {Intelligence}},
	publisher = {Springer US},
	author = {Beurle, R. L.},
	editor = {Muses, C. A. and M.D, Conference Chairman W. S. McCulloch},
	month = jan,
	year = {1962},
	note = {00009},
	keywords = {Artificial Intelligence (incl. Robotics)},
	pages = {19--42}
}
@techreport{cofre:hal-00861397,
  TITLE = {{Exact computation of the Maximum Entropy Potential of spiking neural networks models}},
  AUTHOR = {Cofre, Rodrigo and Cessac, Bruno},
  URL = {https://hal.inria.fr/hal-00861397},
  NOTE = {working paper or preprint},
  YEAR = {2014},
  MONTH = May,
  PDF = {https://hal.inria.fr/hal-00861397/file/paper.pdf},
  HAL_ID = {hal-00861397},
  HAL_VERSION = {v4},
}
@techreport{vasquez:inria-00574954,
  TITLE = {{Parametric Estimation of Gibbs distributions as general Maximum-entropy models for the analysis of spike train statistics.}},
  AUTHOR = {Vasquez, Juan Carlos and Vi{\'e}ville, Thierry and Cessac, Bruno},
  URL = {https://hal.inria.fr/inria-00574954},
  NOTE = {This work corresponds to an extended and revisited version of a previous Arxiv preprint, submitted to HAL as http://hal.inria.fr/inria-00534847/fr/},
  TYPE = {Research Report},
  NUMBER = {RR-7561},
  PAGES = {1-54},
  YEAR = {2011},
  MONTH = Mar,
  KEYWORDS = {Spike train analysis ; Higher-order correlation ; Statistical Physics ; Gibbs Distributions ; Maximum Entropy},
  PDF = {https://hal.inria.fr/inria-00574954/file/RR-7561.pdf},
  HAL_ID = {inria-00574954},
  HAL_VERSION = {v1},
}

@article{cessac_discrete_2008,
	title = {A discrete time neural network model with spiking neurons. {Rigorous} results on the spontaneous dynamics},
	volume = {56},
	url = {https://hal.inria.fr/inria-00423350},
	abstract = {We derive rigorous results describing the asymptotic dynamics of a discrete time model of spiking neurons introduced in {\textbackslash}cite\{BMS\}. Using symbolic dynamic techniques we show how the dynamics of membrane potential has a one to one correspondence with sequences of spikes patterns (``raster plots''). Moreover, though the dynamics is generically periodic, it has a weak form of initial conditions sensitivity due to the presence of a sharp threshold in the model definition. As a consequence, the model exhibits a dynamical regime indistinguishable from chaos in numerical experiments.},
	number = {3},
	urldate = {2016-08-03},
	journal = {Journal of Mathematical Biology},
	author = {Cessac, Bruno},
	year = {2008},
	note = {00004 
56 pages, 1 Figure, to appear in Journal of Mathematical Biology},
	pages = {311--345},
	file = {HAL Snapshot:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/8NGXNG6F/inria-00423350.html:text/html}
}

@book{Bengio:2009,
	address = {Hanover, Mass.},
	title = {Learning {Deep} {Architectures} for {AI}},
	isbn = {978-1-60198-294-0},
	abstract = {Can machine learning deliver AI? Theoretical results, inspiration from the brain and cognition, as well as machine learning experiments suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one would need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers, graphical models with many levels of latent variables, or in complicated propositional formulae re-using many sub-formulae. Each level of the architecture represents features at a different level of abstraction, defined as a composition of lower-level features. Searching the parameter space of deep architectures is a difficult task, but new algorithms have been discovered and a new sub-area has emerged in the machine learning community since 2006, following these discoveries. Learning algorithms such as those for Deep Belief Networks and other related unsupervised learning algorithms have recently been proposed to train deep architectures, yielding exciting results and beating the state-of-the-art in certain areas. Learning Deep Architectures for AI discusses the motivations for and principles of learning algorithms for deep architectures. By analyzing and comparing recent results with different learning algorithms for deep architectures, explanations for their success are proposed and discussed, highlighting challenges and suggesting avenues for future explorations in this area.},
	language = {English},
	publisher = {Now Publishers Inc},
	author = {Bengio, Yoshua},
	month = oct,
	year = {2009},
	note = {02705}
}

@article{Deng:2014,
	title = {Deep {Learning}: {Methods} and {Applications}},
	volume = {7},
	issn = {1932-8346, 1932-8354},
	shorttitle = {Deep {Learning}},
	url = {http://nowpublishers.com/articles/foundations-and-trends-in-signal-processing/SIG-039},
	doi = {10.1561/2000000039},
	language = {en},
	number = {3-4},
	urldate = {2016-07-28},
	journal = {Foundations and Trends® in Signal Processing},
	author = {Deng, Li},
	year = {2014},
	note = {00003},
	pages = {197--387}
}

@article{hstad_power_1991,
	title = {On the power of small-depth threshold circuits},
	volume = {1},
	issn = {1016-3328, 1420-8954},
	url = {http://link.springer.com/10.1007/BF01272517},
	doi = {10.1007/BF01272517},
	language = {en},
	number = {2},
	urldate = {2016-08-03},
	journal = {Computational Complexity},
	author = {Håstad, Johan and Goldmann, Mikael},
	month = jun,
	year = {1991},
	note = {00000},
	pages = {113--129}
}

@article{farabet_learning_2013,
	title = {Learning {Hierarchical} {Features} for {Scene} {Labeling}},
	volume = {35},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6338939},
	doi = {10.1109/TPAMI.2012.231},
	number = {8},
	urldate = {2016-07-28},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Farabet, Clement and Couprie, Camille and Najman, Laurent and LeCun, Yann},
	month = aug,
	year = {2013},
	note = {00578},
	pages = {1915--1929}
}

@article{martens_learning_2016,
	title = {Learning {Recurrent} {Neural} {Networks} with {Hessian}-{Free} {Optimization}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.222.6736},
	abstract = {CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep Teregowda): In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-theart method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of Schraudolph (2002) which is used within the HF approach of Martens. 1.},
	urldate = {2016-08-03},
	author = {Martens, James and Sutskever, Ilya},
	file = {Snapshot:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/R57ZZZHH/summary.html:text/html}
}

@article{cessac_view_2010,
	title = {A {View} of {Neural} {Networks} as {Dynamical} {Systems}},
	volume = {20},
	issn = {0218-1274, 1793-6551},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218127410026721},
	doi = {10.1142/S0218127410026721},
	language = {en},
	number = {06},
	urldate = {2016-08-03},
	journal = {International Journal of Bifurcation and Chaos},
	author = {Cessac, B.},
	month = jun,
	year = {2010},
	note = {00020},
	pages = {1585--1629}
}

@article{rajan_eigenvalue_2006,
	title = {Eigenvalue {Spectra} of {Random} {Matrices} for {Neural} {Networks}},
	volume = {97},
	issn = {0031-9007, 1079-7114},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.97.188104},
	doi = {10.1103/PhysRevLett.97.188104},
	language = {en},
	number = {18},
	urldate = {2016-06-28},
	journal = {Physical Review Letters},
	author = {Rajan, Kanaka and Abbott, L. F.},
	month = nov,
	year = {2006},
	note = {00100},
	annote = {Semble intéressant pour gérer les valeurs propres d'un RNN tout en gardant des neurones excitateurs et inhibiteurs "séparés" (qui ne sont pas les deux en même temps).},
	file = {Rajan_PRL_2006.pdf:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/X4MSD53X/Rajan_PRL_2006.pdf:application/pdf}
}

@inproceedings{siegelmann_turing_1991,
	title = {Turing {Computability} {With} {Neural} {Nets}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.8383},
	abstract = {CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep Teregowda): . This paper shows the existence of a finite neural network, made up of sigmoidal neurons, which simulates a universal Turing machine. It is composed of less than 10 5 synchronously evolving processors, interconnected linearly. High-order connections are not required. 1. Introduction This paper addresses the question: What ultimate limitations, if any, are imposed by the use of neural nets as computing devices? In particular, and ignoring issues of training and practicality of implementation, one would like to know if every problem that can be solved by a digital computer is also solvable --in principle-- using a net. This question has been asked before in the literature. Indeed, Jordan Pollack ([7]) showed that a certain recurrent net model --which he called a "neuring machine," for "neural Turing"-- is universal. In his model, all neurons synchronously update their states according to a quadratic combination of past activation values. In general, one calls high-order nets those in...},
	urldate = {2016-08-03},
	author = {Siegelmann, Hava T. and Sontag, Eduardo D.},
	year = {1991},
	file = {Snapshot:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/U9Z4HQ7V/summary.html:text/html}
}

@inproceedings{cessac_using_2012,
	title = {Using event-based metric for event-based neural network weight adjustment},
	url = {https://hal.inria.fr/hal-00755345/document},
	abstract = {The problem of adjusting the parameters of an event-based network model is addressed here at the programmatic level. Considering temporal processing, the goal is to adjust the network units weights so that the outcoming events correspond to what is desired. The present work proposes, in the deterministic and discrete case, a way to adapt usual alignment metrics in order to derive suitable adjustment rules. At the numerical level, the stability and unbiasness of the method is verified.},
	language = {en},
	urldate = {2016-08-03},
	publisher = {Louvain-La-Neuve : I6doc.com},
	author = {Cessac, Bruno and Salas, Rodrigo and Viéville, Thierry},
	month = apr,
	year = {2012},
	note = {00000},
	pages = {18 pp},
	file = {Full Text PDF:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/5KXB4WMC/Cessac et al. - 2012 - Using event-based metric for event-based neural ne.pdf:application/pdf;Snapshot:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/CRRZ65R8/hal-00755345v1.html:text/html}
}

@article{rajan_recurrent_2016,
	title = {Recurrent {Network} {Models} of {Sequence} {Generation} and {Memory}},
	volume = {90},
	issn = {0896-6273},
	url = {http://www.cell.com/article/S0896627316001021/abstract},
	doi = {10.1016/j.neuron.2016.02.009},
	abstract = {Sequential activation of neurons is a common feature of network activity during a variety of behaviors, including working memory and decision making. Previous network models for sequences and memory emphasized specialized architectures in which a principled mechanism is pre-wired into their connectivity. Here we demonstrate that, starting from random connectivity and modifying a small fraction of connections, a largely disordered recurrent network can produce sequences and implement working memory efficiently. We use this process, called Partial In-Network Training (PINning), to model and match cellular resolution imaging data from the posterior parietal cortex during a virtual memory-guided two-alternative forced-choice task. Analysis of the connectivity reveals that sequences propagate by the cooperation between recurrent synaptic interactions and external inputs, rather than through feedforward or asymmetric connections. Together our results suggest that neural sequences may emerge through learning from largely unstructured network architectures.},
	language = {English},
	number = {1},
	urldate = {2016-06-29},
	journal = {Neuron},
	author = {Rajan, Kanaka and Harvey, Christopher D. and Tank, David W.},
	month = apr,
	year = {2016},
	note = {00005},
	pages = {128--142},
	file = {Snapshot:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/SQPXED77/S0896-6273(16)00102-1.html:text/html}
}

@book{cun_theoretical_1988,
	title = {A {Theoretical} {Framework} for {Back}-{Propagation}},
	abstract = {Among all the supervised learning algorithms, back propagation (BP) is probably...},
	author = {Cun, Yann Le},
	year = {1988},
	file = {Citeseer - Full Text PDF:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/TPQMWK46/Cun - 1988 - A Theoretical Framework for Back-Propagation.pdf:application/pdf;Citeseer - Snapshot:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/HQA7PCN9/summary.html:text/html}
}

@ARTICLE{ChenJain1994,
author={D. S. Chen and R. C. Jain},
journal={IEEE Transactions on Neural Networks},
title={A robust backpropagation learning algorithm for function approximation},
year={1994},
volume={5},
number={3},
pages={467-479},
keywords={backpropagation;convergence of numerical methods;feedforward neural nets;function approximation;iterative methods;backpropagation;convergence;function approximation;input-output mappings;iteration time;learning algorithm;multilayer feedforward neural networks;nonlinear cascade;objective function;Approximation algorithms;Backpropagation algorithms;Feedforward neural networks;Multi-layer neural network;Neural networks;Noise robustness;Noise shaping;Parametric statistics;Shape;Training data},
doi={10.1109/72.286917},
ISSN={1045-9227},
month={May},}

@article{Stuartetal1997,
title = "Action potential initiation and backpropagation in neurons of the mammalian CNS",
journal = "Trends in Neurosciences",
volume = "20",
number = "3",
pages = "125 - 131",
year = "1997",
issn = "0166-2236",
doi = "https://doi.org/10.1016/S0166-2236(96)10075-8",
url = "http://www.sciencedirect.com/science/article/pii/S0166223696100758",
author = "Greg Stuart and Nelson Spruston and Bert Sakmann and Michael Häusser",
keywords = "dendrites",
keywords = "central nervous system",
keywords = "active propogation",
keywords = "sodium channels",
keywords = "synaptic integration",
keywords = "neurons"
}
@incollection{HintonMcClelland1988,
title = {Learning Representations by Recirculation},
author = {Hinton, Geoffrey E and James L. McClelland},
booktitle = {Neural Information Processing Systems},
editor = {D. Z. Anderson},
pages = {358--366},
year = {1988},
publisher = {American Institute of Physics},
url = {http://papers.nips.cc/paper/78-learning-representations-by-recirculation.pdf}
}
@ARTICLE{OReilly1996recirculation,
author={R. C. O'Reilly},
journal={Neural Computation},
title={Biologically Plausible Error-Driven Learning Using Local Activation Differences: The Generalized Recirculation Algorithm},
year={1996},
volume={8},
number={5},
pages={895-938},
keywords={},
doi={10.1162/neco.1996.8.5.895},
ISSN={0899-7667},
month={July},}

@article{Pineda1987,
  title = {Generalization of back-propagation to recurrent neural networks},
  author = {Pineda, Fernando J.},
  journal = {Phys. Rev. Lett.},
  volume = {59},
  issue = {19},
  pages = {2229--2232},
  numpages = {0},
  year = {1987},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.59.2229},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.59.2229}
}
