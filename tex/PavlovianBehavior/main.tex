\documentclass[a4,12pt]{article}
\newif\ifRR
\RRfalse % \RRfalse or \RRtrue

\ifRR
\usepackage[a4paper]{geometry}
\usepackage{RR}
\graphicspath{{./rr-sty/}}
\else
\usepackage[top=1.5cm,right=4cm,bottom=2cm,left=1.5cm]{geometry}
\fi
\usepackage[T1]{fontenc} \usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{graphicx} 
\usepackage{hyperref}
\usepackage{amsmath}\usepackage{amsfonts}
\newcommand{\deq}{\stackrel {\rm def}{=}}
\newcommand{\eqline}[1]{\\\centerline{$#1$}\\} 
\newcommand{\hr}{\\---------------------------------------------------}
\newcommand{\tab}{\\\hspace{5mm}} 

\usepackage{color}
\definecolor{fred}{RGB}{32,64,128}\newcommand{\fred}[1]{{\color{fred}{{\tt @fred:} #1}}}
\definecolor{bhargav}{RGB}{51, 153, 255}\newcommand{\bhargav}[1]{{\color{bhargav}{{\tt @bhargav:} #1}}}
\definecolor{vthierry}{RGB}{80,0,120}\newcommand{\vthierry}[1]{{\color{vthierry}{{\tt @vthierry:} #1}}}

\ifRR
\RRNo{XXXX}
\RRdate{December 2017}
\RRauthor{\thanks[affil]{Mnemosyne team, INRIA Bordeaux}
Thierry Vi\'eville \thanksref{affil} \and
Bhargav teja Nallapu \thanksref{affil} \and
Fr\'ed\'eric Alexandre \thanksref{affil}}
\authorhead{Nallapu \& Alexandre \& Vi\'eville}
\RRtitle{../..}
\RRetitle{Using algorithmic erstaz for brain modeling}
\titlehead{algorithmic ersatz}
\RRresume{../..}
\RRabstract{../..}
\RRmotcle{../..}
\RRkeyword{../..}
\RRprojet{Mnemosyne}
\RCBordeaux
\fi

\begin{document}
\ifRR
\makeRR 
\else
\title{Algorithmic ersatz for brain modeling}
\author[1]{Fr\'ed\'eric Alexandre}
\author[1]{Bhargav teja Nallapu}
\author[1]{Thierry Vi\'eville}
\affil[1]{Mnemosyne team, INRIA Bordeaux}
\maketitle
\begin{abstract}.../...\end{abstract}
\fi

\iffalse

\section{Introduction}

\paragraph{The brain as interacting memories.}

A systemic description of the brain has been proposed \cite{alexandre:hal-01246653}, as a general framework for integrating specific models in computational neuroscience (e.g., \cite{gershman2010context,koechlin2007information,balleine2006parallel,carrere:hal-01145790}) in order to encounter for global information flows and cognitive functions. The basic modeling assumption is to describe cognition as a dynamical system of memories in interaction, e.g., acting to provide information to others, or controlling the convergence of some common process. Regarding the functional organization of the brain, sensorimotor loops and structures controlling the behavior of the body in the environment are emphasized, including regarding the emergence of motivated behavior \cite{cardinal2002emotion,windridge2017emergent}. In an enactive view, the brain has to elaborate loops with the internal and the external environments and to ensure their stability for the general goal of survival.
 
Considering interacting memories leads to a description of the brain as an architecture of learning systems dedicated to autonomous learning. Within this scope, targeting Pavlovian and instrumental conditioning, it can be useful to revisit classical reinforcement learning \cite{redish2007reconciling}, in link with semantic memory in the posterior cortex, episodic memory in the hippocampus, memory of rules (beyond working memory) in the frontal cortex \cite{domenech2015executive,OReilly2014GoalDrivenCI}.

\paragraph{The data complexity challenge.}

In a nutshell, at the present stage of the state of the art, for both existing models and experimental paradigms (as those quoted previously), all state variables almost always take value in some finite list of items. On one hand this is due to the fact that tractable models of learning, e.g., reinforcement learning \cite{sutton1998reinforcement} consider typically finite Markov decision process, thus only finite enumeration of values. On the other hand, experimental protocols must be well-defined with precise qualitative variables in this context, the data is thus formalized as a enumeration of options. 

There is little chance that such model scales up to realistic situations. The external environment is finite but unbounded, i.e., huge. And it is not a simple itemâ€™s heap, but a structured universe. Furthermore this structure, at least partially,  has to be inferred. This argument also concerns continuous representations, when they simply rely on a ``data vector'', without considering less trivial sate space structure.

As far neural coding is concerned, continuous representations are mainly related to sensory input (e.g. analog neurons without spikes in the retina \cite{citeulike:7955297}) or motor output (as being related to interactions with continuous quantities in the external world). Inside the nervous system, a unit (i.e., a small group of neurons, such as a cortical micro-column) encodes for a given quantity, a given quantity being represented by a spatial distributed modal representation, during a temporal window in link with attentional processes, while such coding is sparsed when the neural system is adapted to the stimulus (see, e.g. \cite{citeulike:4194318}). In a nutshell, the information coding is thus sampled.

A step further, the brain is embodied in a body, and this internal system (including the peripheral nervous system) can not reasonably be reduced to a finite enumeration of qualitative states. Moreover, the body is already adapted to the environment, interacting with it, implementing non trivial processes. As a consequence the brain must have a non-trivial representation of the body and its related embodied processes in order to generate pertinent behaviors.

An obvious but useful additional remark is that the brain is not directly interacting with the environment but with the body withing this environment. This means interacting through encoded external and internal (i.e., body) inputs, while providing output decoded before being put in action. 

These are the reasons why we would like to revisit usual brain models in order to discuss how they can scale to more complex data representation.

\paragraph{The algorithmic complexity challenge.}

In deep link with the previous data complexity challenge, is the complexity of the ``learning problem'', from early studies in the field \cite{bush-mosteller:57}, to formalized concepts in the sense of being probably approximately correct \cite{valiant2013probably}, yielding statistical learning \cite{vapnik:98,cucker-smale:01}. As soon as the task is not trivial, exact deterministic learning is in practice intractable, except in some precise cases (e.g., \cite{angluin1983inductive}). This challenge has been replaced by obtaining the desired result with a reasonable probability, given a tractable algorithm. This last notion is often related to polynomial complexity (including randomized polynomial time), but a the concrete level the situation may be more complex (considering algorithm with efficient average complexity even if not strictly polynomial, or avoiding polynomial algorithms with huge calculation time given the data sizes).

 Obviously, depending on the problem decomposition, or on the information coding inference capabilities differ. However, in all cases, one key issue is the no-free-lunch theorem\footnote{The no-free-lunch theorem for machine learning states that if an algorithm performs well on a certain class of problems, then it necessarily pays for that with degraded performance on the set of all remaining problems, up to performing no better than blind search if considering ``all'' possible problems.}, learning being usually defined as a capability to solve a wide class of problems, e.g. considering autonomous behavior. This means that to be efficient we must define autonomous behavior, e.g., survival, not as universal capability, but ``only'' as the capability to survive.

Another key issue is the big data necessity. Efficient supervised learning set counts millions of data. One may consider this order of magnitude compatible with phylogenetic learning, while short-term learning in a real life situation would be more related to few-shot transfer learning \cite{Ruder2017Transfer}. This view is defeated by the fact that complex learning task must be efficient using only a few samples, including ruptures with respect to what has been previously learned. As a consequence, micro-data learning is a real issue \cite{mouret2016micro}, even if theoretically not efficient with respect to general learning criteria of statistical learning.

These are the reasons why we would like to revisit usual brain model algorithms, taking these issues into account.

\paragraph{The notion of survival.}

In fact biological systems often fails, e.g. have limited generalization capability or learning performances, and the utopic vision that they are ``very general autonomous system'' is questionable: They may more ``adapt'' than ''learn'', i.e. change some parameters of some versatile behaviors, instead of ``inventing'' new behaviors.

To make this point precise, one track has been formalized by, e.g., \cite{friston2016active2} (reusing former results in control theory \cite{haddad1981monotone}): Survive means being able to maintain the vital variables present and future values within acceptable bounds, thus to avoid ``surprise'' (in order to guaranty being able to stay in the bounds). This is obtained via some pertinent analysis of the causes of the observed input \cite{schwartenbeck2013exploration}. On the reverse, as soon as the surprise is minimized within the survival domain, it appears that the survival job is done. No more is required to optimize the survival, and the goal directed behavior is entirely specified by this functional objective. The key aspect of this framework is that authors take the epistemological risk to reduce this general principle to a precise formalism, a variational formulation of self-organization in biological systems, which is falsifiable \cite{popper2005logic} by certain aspects, with an effective functional description of the brain processing within this theory.

Here we would like to revisit this notion of survival task considering a different level of data representation. Clearly a distinction has to be made between phylogenetical disruption where a new structure appears or a qualitative change in the system architecture, or its learning strategy occurs with respect to adaptation considering the same hardware but parameter adaptation. Here we only consider this latter issue.

\paragraph{The modeling levels of description.}

Taking the usual Marr and Poggio levels of analysis \cite{marr1976understanding} into account, we are going to reformulate it here as follow, shifting the scope of the higher level and the separating the intermediate level:
\\ -1- Functional level: What does the system do (e.g.: what problems does it solve or overcome) and similarly, why does it do these things.
\\ -2a- Representational level: How does the system encode what it knows and it does, specifically, what representations does it use.
\\ -2b- Algorithmic level : What processes does it employ to build and manipulate these representations.
\\ -3- Implementation level: how can such representations and algorithms be biologically plausibly implemented.

Ideally the functional level is to be described via ontology, i.e. formal objects with properties and relations between objects. The main challenge is to not only consider ``human words'' (i.e., a phenomenological description) but also a description with some guaranty to be well defined, and that can be formally manipulated. 

At the implementation level we consider as biologically plausible implementation a distributed dynamical system with only local computations and adaptation (i.e., learning) rules, but not necessarily build from ``artificial neurons assemblies''. This key point is going to be discussed in the sequel.

The main evolution here is to consider the data representation level as a major issue. Important enough to be clearly addressed before the algorithmic issue (in the causal sense). As an example, when performing a complex task, one data representation is the specialization of the abstract task (i.e., considering the representation of the task as a trajectory in the state space), as discussed in \cite{gaussier-revel-etal:02} in link with the so called "place cells" of the hippocampus. This design choice explains non trivial biological observations related to complex conditioning acquisition and memory tasks, and yields generic biologically plausible algorithms of motor programs \cite{connolly-grupen:93,connolly-burns:93,vieville:06e}.
 
Furthermore, the central claim of this approach is to propose the algorithmic level of representation to be described by ... algorithms. This includes using imperative paradigms (i.e., usual program descriptions), but also declarative paradigms (e.g., rules or constraints). On side effect is indeed to provide simplified models of brain structures (e.g., the hippocampus as ``declarative memory'') without getting into details of biologically plausible implementation. This is mandatory to model the brain as a whole. However, the main goal is to better specify what ``we are talking about'' and the assumption is that using algorithmic ersatz for brain modeling is an effective and well-defined intermediate surrogate allowing to make a well defined link between the functional and the implementation levels of description. 

\paragraph{Paper description.} In the next data modeling section we are going to set up the data framework at both a the symbolic and numeric level. Then in the data container section we start proposing a specification of what is expected from active memories. This will allow us to further define generic algorithmic functions for these memories. We then will show how this may be used to model in a integrated manner Pavlovian and operant, say pavloperant, behaviors.

\fi

\iftrue

\section{Data modeling}

  As discussed in the introduction, 

\section{Data container}

\section{Generic functions}

\section{Application to pavlovian behaviors}

\section{Conclusion}

\appendix

\section{Data type definition and numerical extended mapping}

The following data-type are considered in our context. For each data we provide an informal and formal definition, the main data-type operation and the related numerical extended mapping.

\subsection*{Enumeration} 

Enumeration corresponds to a data with a fixed finite unordered $n$ values set, e.g. {\tt \{false, true\}} or {\tt \{you, her, me\}}. It is formally defined by the value set. Each value $x$ as an arbitrary index $i_x \in \{0, n\}$. It maps onto a standard simplex of degree $n$. 

More formally, the data-type $E$ is defined for $n+1$ data-type value by its degree {\tt n} and a one-to-one correspondence between each value defined as a non-empty string and its related arbitrary index. 

For index not in $\{0, n\{$ the related value is the {\tt empty} string value, i.e. the ``null'' value or ``undefined'' value. It means that each enumeration data has a default undefined value, and that notion of empty, null or undefined values corresponds to the same object in this context.

A data value $x$ is represented by the related arbitrary value index $i_x \in \{0, n\}$, and its numerical extended mapping corresponds to $n$ real numbers ${\bf x} = (x_0, \cdots, x_{n})$, with $\sum_{i = 0}^{n} x_i = 1, x_i \in [0, 1]$, i.e., the barycentric coordinate system of the value on the data type related simplex, and the correspondence writes:
\eqline{i_x = \mbox{argmax}_i \; x_i \mbox{ while } x_i = \delta_{i_x = i}}
in words the value index corresponds to the highest barycentric coordinate value, or equivalently the projection onto the simplex closest vertex, while the barycentric coordinate corresponds to unitary value on the coordinate corresponding to the value index.

A data value $x$ is also represented by the related string corresponding to the value index. At this this level of representation, a string normalization function {\tt canonicalize}, transforming text into a single canonical form has to be defined.

On enumeration data, beyond getting (i.e., reading) and setting (i.e., writing) a value, the main operation corresponds to comparing two values.
\\- The {\tt equality} is the fact that two related value indexes are the same, which corresponds to the discrete ultra-metric of this finite data space.
\\- More generally, a \href{https://en.wikipedia.org/wiki/Metric\_(mathematics)}{\tt metric} between two values and/or a \href{https://en.wikipedia.org/wiki/Intrinsic_metric}{\tt path} between two values can also be defined. By default the standard simplex barycentric coordinate path in link with the \href{https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures}{total variation} related intrinsic metric is defined:
\eqline{p({\bf x}, {\bf x}', \theta \in [0, 1]) = \theta \, {\bf x}' + (1 - \theta) \, {\bf x}', \mbox{ with } d({\bf x}, {\bf x}') = \frac{1}{2} \, \sum_i |x_i - x'_i| \in [0, 1],}
while application dependent {\tt path} and/or {\tt metric} may instantiate the semantic related to such data-type.

A step further, we defined the notion of {\tt restriction} as positive normalized function $r(x) \in [0, 1]$ of a value, with the semantic that it is required to have $r(x) = 0$ in order to verify the contraint, while it might approximately be verified\footnote{{\bf Combination of restrictions.} Obsviously restrictions may be easily combined either considering a vectorial restriction stacking the scalar restrictions, or any scalar combination, e.g. the product of restrictions vanishes if at least one is verified, as in a disjunction, the sum of restrictions vanishes of all are verified, as in a conjunction, while, e.g., $\max(0, \nu - r(x))$ vanishes if the restriction is not verified, up to some threshold $\nu$. All these design choice are application dependent.}. Given a metric, we can define the exact (or at least approximate) {\tt projection} of a value on a contraint, providing that the restriction error function $r(x)$ and the related derivative in the barycenter coordinates $\partial_i r(x)$ are defined\footnote{{\bf Total variation metric projection.} Let us define the projection ${\bf x}_p$ of a value ${\bf x}^0$ on the vectorial restriction ${\bf r}({\bf x})$ as:
\eqline{{\bf x}_p = \mbox{argmax}_{\bf x} d({\bf x}, {\bf x}^0), {\bf r}({\bf x}) = 0}
which, for the default total variation metric, is the fixed point of the series
\eqline{\begin{array}{rcl} {\bf x}^{n+1}_i &=& (1-\gamma_n) \, {\bf x}^{n} + \gamma_n \, \left({\bf x}^0_i - |x^{n}_i - x^0_i| \, \partial_i {\bf r}({\bf x}^{n}) \, \left[ \sum_i |x^{n}_i - x^0_i| \, \partial_i {\bf r}({\bf x}^{n}) \, \partial_i {\bf r}({\bf x}^{n})^T \right]^\dagger \, \right. \\ && \left. \left({\bf r}({\bf x}^{n}) - \sum_i \partial_i {\bf r}({\bf x}^{n}) \, \left(x^{n}_i - x^0_i\right)\right)\right) \\ \end{array}}
for some $\gamma_n = \mbox{argmin}_\gamma |{\bf r}({\bf x}^{n+1})|$ while $\dagger$ stands for the matrix pseudo-inverse. As derived in, e.g., \cite{vieville:inria-00074888} for a ${\cal L}^2$ metric we obtain a 2nd order algorithm as soon as there is convexity with a 1st order fallback otherwise. The key-point of the present derivation is that it easily generalizes to the total variation metric as known from the litterature and made explicit here.}. If an applicaton data-type redefines the {\tt metric} it must redefined the {\tt projection} operator of a restriction.
Projecting a data value ${\bf x}$ is to project onto the closest data-type value of index $i_x$ is the basic example of such mechanism.

\subsection*{Specific and generalized enumeration} 

The {\tt empty} data-type corresponds to an enumeration with no value, i.e., the simplex with no point, or simplex of degree $-1$.

A {\tt const} data-type corresponds to an enumeration with only one value, i.e., a simplex of degree 0.

The {\tt bool} data-type corresponds to the {\tt \{false, true\}} enumeration, i.e., a simplex of degree 1, in one-to-one correspondence, by convention, with the {\tt \{0, 1\}} enumeration (in this order), the {\tt \{no, yes\}} enumeration or any other, with a similar meaning.

A {\tt mutable-enumeration} is a data-type is able to increase its value set, with time. This simply means that an additional data-type method is defined to {\tt add} such value. Previous data value are still well-defined, but with the caveat that previous data could better correspond to the new data-type value than a previous one. We do not consider deleting because this would generate data with undefined values, but may introduce a {\tt merge} method that would make two value identical, without undefining data value.

An {\tt ordered enumeration} is a data-type with an additional {\tt comparator} compatible with the {\tt equality}, defined as a function:
\eqline{c(x, x') \in \left\{\begin{array}{lr} -1 & x < x' \\ 0 & x = x' \\ 1 & x > x' \\ ? & \mbox{not comparable} \\ \end{array} \right.}
considering the equality function only, being a special case where the comparator $c(x, x') \in \{0, ?\}$. Here the order can be a partial or a total order. One order is the {\tt fuzzy} comparison between two values, comparing which one is the closest to its related data-type value.

More generally, since any other data-type maps onto a finite set, any other data-type maps onto an enumeration.

\subsection*{Quantitative value} 

In numerical algorithms a quantitative quantity $q$ is always reasonably defined within bounds $q \in [q_{\min}, q_{\max}]$, with a default value $q_0$ (the middle of the interval, if no better knowledge), and with a precision $q_\epsilon = \frac{q_{max} - q_{min}}{q_N}$, for some integer $q_N$, and such that:
\eqline{|q' - q''| < q_\epsilon \;\;\Leftrightarrow\; \mbox{$q'$ and $q''$ are indistinguishable},}
while this specification can be linear or logarithmic (i.e. apply on $\log(q')$).

It is important to notice that such information is always available in practice, e.g., a pupil ruler has a precision of 1mm, with bounds defined by the ruler length; a location in an image is within the image sizes with a standard precision of one pixel; a car velocity is 0 by default and bounded by the vehicle performances, while the precision is not better than 1 km/h; and so on.

One key point is that such quantitative value is in one-to-one correspondance with a fully ordered enumeration of index $x$, since we may consider the following {\tt sampling}:
\eqline{q_i(i_{x'}) = q_{min} + i_{x'} \, q_\epsilon, \;\;\; i_{x'} \in \{0, q_N\}, \mbox{ with } i_q(q') = \mbox{trunc}\left(\frac{q' - q_{min}}{q_\epsilon}\right)}
with the following coherent properties:
\eqline{|q' - q''| < q_\epsilon \Leftrightarrow i_q(q') = i_q(q''), i_q(q_x(i_{x'})) = i_{x'}, |q_i(i_q(q')) - q'| <  q_\epsilon.}

The {\tt deterministic} simplex representation considers that the uncertainty is only related to the sampling, we thus obtain:
\eqline{x_i(q') = \delta_{|q' - q_i(i)| < q_\epsilon} \frac{|q' - q_i(i)|}{q_\epsilon},  \mbox{ with } q' = \sum_i x_i(q') \, q_i(i)}
as easily verified from a few algebra, with no need to explictly store barycenter coordinates: Only the two barycenter values corresponding to simplex vertices adjacent to the true quantitive value are not zero, and there is a trivial one-to-one correspondence between $q'$ and the related ${\bf x}$.

The {\tt stochastic} simplex representation now consider that the quantitative itself suffers from uncertainty. In such a case, we now consider larger $q_\epsilon$, i.e, smaller $q_N$ and stores explicitly all barycenter as probabilities for the value to have the $q_i(i)$ value. In such a design choice, we still have by construction $q' = \sum_i x_i(q') \, q_i(i)$ but must introduce another ingredient in order to define the barycenter, i.e., a {\em kernel} $\rho_i()$, so that:
\eqline{x_i(q') = \rho_i\left(\frac{q' - q_{min}}{q_{min} - q_{max}}\right), \mbox{ with still } q' = \sum_i x_i(q') \, q_i(i),}
these kernels defining a \href{https://en.wikipedia.org/wiki/Partition_of_unity}{partition of unity}, with:
\eqline{\rho_i(u) \geq 0, u \notin [0, 1] \Rightarrow \rho_i(u) = 0, \sum_i \rho_i(u) = 1, u_i(i) = \max_u \rho_i(u), u_i \deq \frac{q_i - q_{min}}{q_{min} - q_{max}} \in [0, 1]}
in words a maximal entropy, bounded distribution, maximal on each vertex. Given this specification, we consider here the maximal entropy choice, i.e., 
\eqline{\rho(u) = \mbox{argmax}_\rho \sum_i \rho_i(u) \, \log_2(\rho_i(u)), \rho_i(0) = 0, \rho_i(1) = 1, \rho(q) \geq 0, \rho'_i(u_i) = 0}





\subsection*{Multidimensional numerical value} 

\subsection*{Composed value} 

\subsection*{Value list} 

\subsection*{Value set} 

\subsection*{Value triplet} 

\subsection*{Value function} 

\fi

{\scriptsize \bibliographystyle{plain} \bibliography{../etc/main,../etc/from-keops,../etc/from-sophia}}

%\tableofcontents

\end{document}

