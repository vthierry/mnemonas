
@article{Drumond2017From,
author = {F. Drumond, Thalita and Vi\'eville, Thierry and Alexandre, Frédéric},
title = {From shortcuts to architecture optimization in deep-learning},
year = {2017}
note = {in preparation for something}
}

@article{topalidou_long_2015,
	title = {A long journey into reproducible computational neuroscience},
	volume = {9},
	issn = {1662-5188},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4350388/},
	doi = {10.3389/fncom.2015.00030},
	journal = {Frontiers in Computational Neuroscience},
	author = {Topalidou, Meropi and Leblois, Arthur and Boraud, Thomas and Rougier, Nicolas P},
	year = {2015},
	pages = {30}
}

@article{Zhang2016Understanding,
    archiveprefix = {arXiv},
    arxivid = {1611.03530},
    author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
    eprint = {1611.03530},
    link = {http://arxiv.org/abs/1611.03530},
    month = {nov},
    title = {Understanding deep learning requires rethinking generalization},
    year = {2016}
}


@article{Fdrumond2017,
   author = {F. Drumond, Thalita and Vi\'eville, Thierry and Alexandre, Fr\'ed\'eric Alexandre},
    title = {Not-so-big data deep learning: a review},
    note ={in preparation},
    year = {2017}
}

@article{Bengio2012Representation,
    archiveprefix = {arXiv},
    arxivid = {1206.5538},
    author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
    doi = {10.1109/TPAMI.2013.50},
    eprint = {1206.5538},
    isbn = {0162-8828 VO - 35},
    issn = {1939-3539},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    link = {http://www.ncbi.nlm.nih.gov/pubmed/23459267},
    number = {8},
    pages = {1798--1828},
    pmid = {23787338},
    title = {Representation Learning: A Review and New Perspectives},
    volume = {35},
    year = {2012}
}

@article{Bengio2016Towards,
    archiveprefix = {arXiv},
    arxivid = {1502.04156},
    author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Lin, Zhouhan},
    doi = {10.1007/s13398-014-0173-7.2},
    eprint = {1502.04156},
    isbn = {9781467398947},
    issn = {0717-6163},
    journal = {arXiv preprint arxiv:1502.0415},
    link = {http://arxiv.org/abs/1502.04156},
    month = {feb},
    pages = {10},
    pmid = {15003161},
    title = {Towards Biologically Plausible Deep Learning},
    year = {2016}
}
@article{Freund2003Efficient,
    author = {Freund, Yoav and Iyer, Raj and Schapire, Robert E and Singer, Yoram},
    journal = {The Journal of machine learning research},
    pages = {933--969},
    publisher = {JMLR. org},
    title = {An efficient boosting algorithm for combining preferences},
    volume = {4},
    year = {2003}
}

@article{Cho-2014,
  author    = {Kyunghyun Cho and
               Bart van Merrienboer and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Fethi Bougares and
               Holger Schwenk and
               Yoshua Bengio},
  title     = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical
               Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1406.1078},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.1078},
  timestamp = {Wed, 02 Jul 2014 08:24:58 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChoMGBSB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Legenstein:2007,
title = "Edge of chaos and prediction of computational performance for neural circuit models ",
journal = "Neural Networks ",
volume = "20",
number = "3",
pages = "323 - 334",
year = "2007",
note = "Echo State Networks and Liquid State Machines ",
issn = "0893-6080",
doi = "http://dx.doi.org/10.1016/j.neunet.2007.04.017",
url = "http://www.sciencedirect.com/science/article/pii/S0893608007000433",
author = "Robert Legenstein and Wolfgang Maass",
keywords = "Neural networks",
keywords = "Spiking networks",
keywords = "Edge of chaos",
keywords = "Microcircuits",
keywords = "Computational performance",
keywords = "Network dynamics ",
abstract = "We analyze in this article the significance of the edge of chaos for real-time computations in neural microcircuit models consisting of spiking neurons and dynamic synapses. We find that the edge of chaos predicts quite well those values of circuit parameters that yield maximal computational performance. But obviously it makes no prediction of their computational performance for other parameter values. Therefore, we propose a new method for predicting the computational performance of neural microcircuit models. The new measure estimates directly the kernel property and the generalization capability of a neural microcircuit. We validate the proposed measure by comparing its prediction with direct evaluations of the computational performance of various neural microcircuit models. The proposed method also allows us to quantify differences in the computational performance and generalization capability of neural circuits in different dynamic regimes (UP- and DOWN-states) that have been demonstrated through intracellular recordings in vivo. "
}
@book{Goodfellow2016Deep,
    author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
    link = {http://www.deeplearningbook.org},
    publisher = {MIT Press},
    title = {Deep Learning},
    year = {2016}
}

@article{Schmidhuber:2015,
author = "J. Schmidhuber",
title = "Deep Learning in Neural Networks: An Overview",
journal = "Neural Networks",
pages = "85-117",
volume = "61",
doi = "10.1016/j.neunet.2014.09.003",
note = "Published online 2014; based on TR arXiv:1404.7828 [cs.NE]",
year = "2015"} 

@inproceedings{He2016Deep,
    archiveprefix = {arXiv},
    arxivid = {1512.03385},
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    eprint = {1512.03385},
    link = {http://arxiv.org/pdf/1512.03385v1.pdf},
    pages = {770--778},
    title = {Deep Residual Learning for Image Recognition},
    year = {2016}
}


@article{Balduzzi:2016,
  author    = {David Balduzzi and
               Muhammad Ghifary},
  title     = {Strongly-Typed Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1602.02218},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.02218},
  timestamp = {Tue, 01 Mar 2016 17:47:25 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BalduzziG16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Gers:2003,
 author = {Gers, Felix A. and Schraudolph, Nicol N. and Schmidhuber, J\"{u}rgen},
 title = {Learning Precise Timing with LSTM Recurrent Networks},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2003},
 volume = {3},
 month = mar,
 year = {2003},
 issn = {1532-4435},
 pages = {115--143},
 numpages = {29},
 url = {http://dx.doi.org/10.1162/153244303768966139},
 doi = {10.1162/153244303768966139},
 acmid = {944925},
 publisher = {JMLR.org},
 keywords = {long short-term memory, recurrent neural networks, timing},
} 

@article{Hochreiter:1997,
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 title = {Long Short-Term Memory},
 journal = {Neural Comput.},
 issue_date = {November 15, 1997},
 volume = {9},
 number = {8},
 month = nov,
 year = {1997},
 issn = {0899-7667},
 pages = {1735--1780},
 numpages = {46},
 url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
 doi = {10.1162/neco.1997.9.8.1735},
 acmid = {1246450},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@incollection{beurle_storage_1962,
	title = {Storage and {Manipulation} of {Information} in {Random} {Networks}},
	copyright = {©1962 Springer Science+Business Media New York},
	isbn = {978-1-4899-6269-0 978-1-4899-6584-4},
	url = {http://link.springer.com.gate1.inist.fr/chapter/10.1007/978-1-4899-6584-4_3},
	abstract = {Innumerable questions can be asked in relation to the brain. How does it work? How should a “thinking machine” work? How is it that there is in the universe the consistency which makes a “thinking machine” possible?},
	language = {en},
	urldate = {2014-10-31},
	booktitle = {Aspects of the {Theory} of {Artificial} {Intelligence}},
	publisher = {Springer US},
	author = {Beurle, R. L.},
	editor = {Muses, C. A. and M.D, Conference Chairman W. S. McCulloch},
	month = jan,
	year = {1962},
	note = {00009},
	keywords = {Artificial Intelligence (incl. Robotics)},
	pages = {19--42}
}

@article{cessac_discrete_2008,
	title = {A discrete time neural network model with spiking neurons. {Rigorous} results on the spontaneous dynamics},
	volume = {56},
	url = {https://hal.inria.fr/inria-00423350},
	abstract = {We derive rigorous results describing the asymptotic dynamics of a discrete time model of spiking neurons introduced in {\textbackslash}cite\{BMS\}. Using symbolic dynamic techniques we show how the dynamics of membrane potential has a one to one correspondence with sequences of spikes patterns (``raster plots''). Moreover, though the dynamics is generically periodic, it has a weak form of initial conditions sensitivity due to the presence of a sharp threshold in the model definition. As a consequence, the model exhibits a dynamical regime indistinguishable from chaos in numerical experiments.},
	number = {3},
	urldate = {2016-08-03},
	journal = {Journal of Mathematical Biology},
	author = {Cessac, Bruno},
	year = {2008},
	note = {00004 
56 pages, 1 Figure, to appear in Journal of Mathematical Biology},
	pages = {311--345},
	file = {HAL Snapshot:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/8NGXNG6F/inria-00423350.html:text/html}
}

@book{Bengio:2009,
	address = {Hanover, Mass.},
	title = {Learning {Deep} {Architectures} for {AI}},
	isbn = {978-1-60198-294-0},
	abstract = {Can machine learning deliver AI? Theoretical results, inspiration from the brain and cognition, as well as machine learning experiments suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one would need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers, graphical models with many levels of latent variables, or in complicated propositional formulae re-using many sub-formulae. Each level of the architecture represents features at a different level of abstraction, defined as a composition of lower-level features. Searching the parameter space of deep architectures is a difficult task, but new algorithms have been discovered and a new sub-area has emerged in the machine learning community since 2006, following these discoveries. Learning algorithms such as those for Deep Belief Networks and other related unsupervised learning algorithms have recently been proposed to train deep architectures, yielding exciting results and beating the state-of-the-art in certain areas. Learning Deep Architectures for AI discusses the motivations for and principles of learning algorithms for deep architectures. By analyzing and comparing recent results with different learning algorithms for deep architectures, explanations for their success are proposed and discussed, highlighting challenges and suggesting avenues for future explorations in this area.},
	language = {English},
	publisher = {Now Publishers Inc},
	author = {Bengio, Yoshua},
	month = oct,
	year = {2009},
	note = {02705}
}

@article{Deng:2014,
	title = {Deep {Learning}: {Methods} and {Applications}},
	volume = {7},
	issn = {1932-8346, 1932-8354},
	shorttitle = {Deep {Learning}},
	url = {http://nowpublishers.com/articles/foundations-and-trends-in-signal-processing/SIG-039},
	doi = {10.1561/2000000039},
	language = {en},
	number = {3-4},
	urldate = {2016-07-28},
	journal = {Foundations and Trends® in Signal Processing},
	author = {Deng, Li},
	year = {2014},
	note = {00003},
	pages = {197--387}
}

@article{hstad_power_1991,
	title = {On the power of small-depth threshold circuits},
	volume = {1},
	issn = {1016-3328, 1420-8954},
	url = {http://link.springer.com/10.1007/BF01272517},
	doi = {10.1007/BF01272517},
	language = {en},
	number = {2},
	urldate = {2016-08-03},
	journal = {Computational Complexity},
	author = {H�stad, Johan and Goldmann, Mikael},
	month = jun,
	year = {1991},
	note = {00000},
	pages = {113--129}
}

@article{farabet_learning_2013,
	title = {Learning {Hierarchical} {Features} for {Scene} {Labeling}},
	volume = {35},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6338939},
	doi = {10.1109/TPAMI.2012.231},
	number = {8},
	urldate = {2016-07-28},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Farabet, Clement and Couprie, Camille and Najman, Laurent and LeCun, Yann},
	month = aug,
	year = {2013},
	note = {00578},
	pages = {1915--1929}
}

@article{martens_learning_2016,
	title = {Learning {Recurrent} {Neural} {Networks} with {Hessian}-{Free} {Optimization}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.222.6736},
	abstract = {CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep Teregowda): In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-theart method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of Schraudolph (2002) which is used within the HF approach of Martens. 1.},
	urldate = {2016-08-03},
	author = {Martens, James and Sutskever, Ilya},
	file = {Snapshot:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/R57ZZZHH/summary.html:text/html}
}

@article{cessac_view_2010,
	title = {A {View} of {Neural} {Networks} as {Dynamical} {Systems}},
	volume = {20},
	issn = {0218-1274, 1793-6551},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218127410026721},
	doi = {10.1142/S0218127410026721},
	language = {en},
	number = {06},
	urldate = {2016-08-03},
	journal = {International Journal of Bifurcation and Chaos},
	author = {Cessac, B.},
	month = jun,
	year = {2010},
	note = {00020},
	pages = {1585--1629}
}

@article{rajan_eigenvalue_2006,
	title = {Eigenvalue {Spectra} of {Random} {Matrices} for {Neural} {Networks}},
	volume = {97},
	issn = {0031-9007, 1079-7114},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.97.188104},
	doi = {10.1103/PhysRevLett.97.188104},
	language = {en},
	number = {18},
	urldate = {2016-06-28},
	journal = {Physical Review Letters},
	author = {Rajan, Kanaka and Abbott, L. F.},
	month = nov,
	year = {2006},
	note = {00100},
	annote = {Semble intéressant pour gérer les valeurs propres d'un RNN tout en gardant des neurones excitateurs et inhibiteurs "séparés" (qui ne sont pas les deux en même temps).},
	file = {Rajan_PRL_2006.pdf:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/X4MSD53X/Rajan_PRL_2006.pdf:application/pdf}
}

@inproceedings{siegelmann_turing_1991,
	title = {Turing {Computability} {With} {Neural} {Nets}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.8383},
	abstract = {CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep Teregowda): . This paper shows the existence of a finite neural network, made up of sigmoidal neurons, which simulates a universal Turing machine. It is composed of less than 10 5 synchronously evolving processors, interconnected linearly. High-order connections are not required. 1. Introduction This paper addresses the question: What ultimate limitations, if any, are imposed by the use of neural nets as computing devices? In particular, and ignoring issues of training and practicality of implementation, one would like to know if every problem that can be solved by a digital computer is also solvable --in principle-- using a net. This question has been asked before in the literature. Indeed, Jordan Pollack ([7]) showed that a certain recurrent net model --which he called a "neuring machine," for "neural Turing"-- is universal. In his model, all neurons synchronously update their states according to a quadratic combination of past activation values. In general, one calls high-order nets those in...},
	urldate = {2016-08-03},
	author = {Siegelmann, Hava T. and Sontag, Eduardo D.},
	year = {1991},
	file = {Snapshot:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/U9Z4HQ7V/summary.html:text/html}
}

@inproceedings{cessac_using_2012,
	title = {Using event-based metric for event-based neural network weight adjustment},
	url = {https://hal.inria.fr/hal-00755345/document},
	abstract = {The problem of adjusting the parameters of an event-based network model is addressed here at the programmatic level. Considering temporal processing, the goal is to adjust the network units weights so that the outcoming events correspond to what is desired. The present work proposes, in the deterministic and discrete case, a way to adapt usual alignment metrics in order to derive suitable adjustment rules. At the numerical level, the stability and unbiasness of the method is verified.},
	language = {en},
	urldate = {2016-08-03},
	publisher = {Louvain-La-Neuve : I6doc.com},
	author = {Cessac, Bruno and Salas, Rodrigo and Viéville, Thierry},
	month = apr,
	year = {2012},
	note = {00000},
	pages = {18 pp},
	file = {Full Text PDF:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/5KXB4WMC/Cessac et al. - 2012 - Using event-based metric for event-based neural ne.pdf:application/pdf;Snapshot:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/CRRZ65R8/hal-00755345v1.html:text/html}
}

@article{rajan_recurrent_2016,
	title = {Recurrent {Network} {Models} of {Sequence} {Generation} and {Memory}},
	volume = {90},
	issn = {0896-6273},
	url = {http://www.cell.com/article/S0896627316001021/abstract},
	doi = {10.1016/j.neuron.2016.02.009},
	abstract = {Sequential activation of neurons is a common feature of network activity during a variety of behaviors, including working memory and decision making. Previous network models for sequences and memory emphasized specialized architectures in which a principled mechanism is pre-wired into their connectivity. Here we demonstrate that, starting from random connectivity and modifying a small fraction of connections, a largely disordered recurrent network can produce sequences and implement working memory efficiently. We use this process, called Partial In-Network Training (PINning), to model and match cellular resolution imaging data from the posterior parietal cortex during a virtual memory-guided two-alternative forced-choice task. Analysis of the connectivity reveals that sequences propagate by the cooperation between recurrent synaptic interactions and external inputs, rather than through feedforward or asymmetric connections. Together our results suggest that neural sequences may emerge through learning from largely unstructured network architectures.},
	language = {English},
	number = {1},
	urldate = {2016-06-29},
	journal = {Neuron},
	author = {Rajan, Kanaka and Harvey, Christopher D. and Tank, David W.},
	month = apr,
	year = {2016},
	note = {00005},
	pages = {128--142},
	file = {Snapshot:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/SQPXED77/S0896-6273(16)00102-1.html:text/html}
}

@book{cun_theoretical_1988,
	title = {A {Theoretical} {Framework} for {Back}-{Propagation}},
	abstract = {Among all the supervised learning algorithms, back propagation (BP) is probably...},
	author = {Cun, Yann Le},
	year = {1988},
	file = {Citeseer - Full Text PDF:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/TPQMWK46/Cun - 1988 - A Theoretical Framework for Back-Propagation.pdf:application/pdf;Citeseer - Snapshot:/user/vthierry/home/.zotero/zotero/fy9peq9c.default/zotero/storage/HQA7PCN9/summary.html:text/html}
}
