\section*{Introduction}

Artificial neural networks can be considered as discrete-time
dynamical systems, performing input-output computation, at the higher
level of generality \cite{siegelmann_turing_1991}. However, only
specific feed-forward or recurrent architectures are considered in
practice, because of weight estimation, as reviewed now.

In the artificial neural network literature, feed-forward networks
parameter learning is a well-solved problem.  For instance, the back-propagation
algorithms, based on specific architectures of multi-layer
feed-forward networks, allows one to propose well-defined
implementation \cite{amit:89}, though it has been shown at the
theoretical and empirical levels, that "shallow" architectures are
 inefficient for representing complex functions
\cite{bengio-lecun:07}.

Deep-networks are specific feed-forward architectures
\cite{bengio-lecun:07} which can have very impressive performances,
e.g.  \cite{farabet_learning_2013}. The key idea
\cite{hstad_power_1991} is that, at least for threshold units with
positive weights, reducing the number of layers induces an exponential
complexity increase for the same input/output function. On the
reverse, it is a reasonable assumption, numerically verified, that
increasing the number of layers yields a compact representation of
input/output functions. One drawback is related to weights supervised
learning in deeper layers, since readout layers may over-fit the
learning set, the remedy being to apply unsupervised learning on
deeper layers (see \cite{Bengio:2009} for an introduction). This
problem desapears with specific architectures such as CNN.

It also remains restrictive by the fact that the architecture is
mainly a pipe-line, including some parallel tracks, while each layer
is a feed-forward network (e.g. a convolutional neural layers) or with
a very specific recurrent connectivity (e.g., restrained Boltzman
machines). In the brain, more general architectures occur (e.g.  with
shortcuts between deeper and lower layers, as it happens in the brain
regarding the pulvinar \cite{citeulike:1590763}) and each layer is a
more general recurrent network (e.g., with short and long range
horizontal connections). Breaking this pipe-line architecture may
overcome the problem of deeper layer weight adjustment.

Feed-forward networks are obviously far from the computational
capacity of recurrent networks. Therefore, specific multi-layer
architectures without recurrent links within a layer and specific
forward/backward connections between layers have been proposed
instead. The first dynamic neural model, the model by Hopfield
\cite{hopfield:82}, appeared much later, and was very specific.
Further solutions include Jordan's network \cite{jordan:86}, Elman's
Networks \cite{elman:90}, Long short term memory (LSTM) by Hochreiter
and Schmidhuber \cite{hochreiter-schmidhuber:97}.

Another track is to consider recurrent networks without supervised
weight adjustment \cite{verstraeten-etal:07}. Units in such
architectures are linear or sigmoid artificial neurons, including
soft-max units, or even spiking neurons. Such network architectures,
such as Echo State Networks \cite{jaeger:03} and Liquid Sate Machines
\cite{maass-etal:02}, are called ``reservoir computing'' (see
\cite{verstraeten-etal:07} for unification of reservoir computing
methods at the experimental level).

In such architectures the recurrent parameters of hidden units is not
explicitly learned, whereas recurrent weights are either randomly
fixed, likely using a sparse connectivity, or adjusted using
unsupervised learning mechanism, without any direct connection with
the learning samples (though the hidden unit statistics, for instance,
is sometimes adjusted in relation with the desired output). In the
case of temporal mechanisms, i.e. using spiking neurons (e.g. in the
model of \cite{paugam-moisy-etal:08}), the unsupervised learning
mechanism of the recurrent weights is a form of synaptic plasticity,
usually STDP (Spike-Time-Dependent Plasticity), a temporal Hebbian
unsupervised learning rule, biologically inspired. It appears that
simple methods yield good results \cite{verstraeten-etal:07}, but
without over-passing recent deep-layer architecture performances
\cite{Deng:2014}.

The general problem of learning recurrent neural networks has also
been widely addressed as reviewed in \cite{cun_theoretical_1988} for
90's studies and in \cite{martens_learning_2016} for recent advances,
and methods exist far beyond basic methods such as backpropagation
through time. 

In the present paper, we revisit the general problem of recurrent
network weight learning, not as it, but because it is
related to modern issues related to both artificial networks and brain
function modeling. Such issues include: Could we adjust the recurrent
weights in a reservoir computing architecture ?  Is it possible to
consider deep-learning architecture, with more general inter and intra
layers connectivity ? Would it be possible to not only use some
specific recurrent architecture as exemplified here, but to learn also
the architecture itself (i.e. learn the weight value and learn if the
connection weight has to be set to zero, cutting the connection) ?

We are not going to address more than weight adjustment in this paper.
For instance learning issues (e.g., boosting
\cite{Freund2003Efficient}) are not within the scope of this paper:
Neither representation learning \cite{Bengio2012Representation}, nor other
complex issues \cite{Goodfellow2016Deep} are considered, this
contribution being only an alternate tool for variational weight
optimization. See \cite{Fdrumond2017} for a recent discussion on 
such issues.

We are also not going to consider biological plausibility in the sense
of \cite{Bengio2016Towards}, but will show that the proposed mehod is
compliant with several distributed biological constraints or
computational properties: local weight adjustment, backward error
propagation, Hebbian like adjustment rules. A more rigourous
discussion about the link with computational neuroscience aspects is
however beyond the scope of this work.

In the next section we choose a notation to state the estimation
problem, and Appendix~\ref{generality} make explicit how this notation
applies to most of the usual frameworks. We then adress the estimation
problem and introduce the modified solution we propose, while
Appendix~\ref{application} further discuss how it can be used for
several estimation problems. In the subsequent section the method is
implemented and numerically evaluated, while
Appendix~\ref{closedforms} expalined why certain estimation problem ar
not considered here because reducing to trivial computation problems,
given an architecture.
