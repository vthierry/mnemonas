\section{Closed forms solution for neural network tasks} \label{closedforms}

Let us make explicit here the type of used units has a strong influence on the difficulty of the task. Here we consider deterministic tasks only. The remark is that tasks considered as quite complex \cite{Hochreiter:1997,Gers:2003,martens_learning_2016} for certain architectures are trivial for others. In particular, the use of AIF neurons dramatically simplifies certain problems. We ilustrate this point here considering deterministic sequence generation and long-term non-linera transform.

\subsubsection*{Deterministic sequence generation}

Let us make explicit the complexity of the task of generating a deterministic time sequence $\bar{o}_n(t), n \in \{0, N_0\{, t \in \{0, T\{$, with a recurrent network of $N \ge N_0$ units of range $R$. This could be, e.g. the Sierpinski sequence\footnote{This corresponds to the \href{https://en.wikipedia.org/wiki/Sierpinski_triangle}{Sierpinski triangle} read from left to right and from top to down in sequence.}, which is deterministic aperiodic, and a function of the previous and $O(\sqrt{t})$ previous samples at time $t$, thus with long term dependency\footnote{The Sierpinski sequence is generated by recurrent equations of the form:
\\ \centerline{$\begin{array}{rcll}
x_0(t) &=& -1 + 2 \, (x_1(t) \mbox{ mod } 2) & x_0(t) \in \{-1, 1\} \\
x_1(t) &=& 1 + \delta_{0 < k_t < l_t < t} \, (x_1(t-l_t) + x_1(t-l_t-1) - 1) & \mbox{Pascal triangle sequence}\\
l_t &=& l_{t-1} + \delta_{k_{t-1} = 0} & l_t = O(\sqrt{t})\\
k_t &=& \delta_{k_{t-1} = l_{t-1}} \, (k_{t-1} + 1) & 0 \leq k_t < l_t\\
\end{array}$}}, or an unpredictable sequence without any algorithm to generate it, unless the copy of all sample (i.e., with a maximal Kolmogorov complexity).

On one hand, $O(N_0)$ independent linear recurrent units of range $R=T$, solves the problem of generating an exact sequence of $N_0 \, T$ samples, in closed form\footnote{{\bf Long range sequence generation.} Let us consider units of the form:
 \\\centerline{$x_n(t) = \sum_{d = 1}^{d = T-1} W_{nd} \, x_n(t-d) + W_{n0}$,}\\ thus with $N_0 \, T$ weights. Since $x_n(t) = 0, t < 0$, providing $\bar{o}_n(1) \neq 0$, we immediately obtain $W_{n0} = \bar{o}_n(1)$ and for $d > 0$: \\ \centerline{$W_{nk} = (\bar{o}_n(k+1) - W_{n0} - \sum_{d = 1}^{d = k-1} W_{nd} \, \bar{o}_n(t-d)) / \bar{o}_n(1)$,} \\ thus a closed-form solution. If $\bar{o}_n(1) = 0$ we simply have to generate the sequence, say, $\bar{o}'_n(t) = \bar{o}_n(t) +1$ and add a second unit of the form $x_n(t) = 1 + x'_n(t)$, using now $2\, N_0$ nodes.}. This solution requires a very large recurrent range, and the numerical precision is limited by the fact errors accumulate along the recurrent calculation.

On the other hand, feed-forward units of range $R=1$ solve explicitly the problem using clock units and readout, with $O(N_0\,T)$ weights. This requires no more than $N_0+T$ units considering binary information\footnote{ {\bf Long sequence generation with delay lines.} Let us consider $N_0$ readout units and $T$ clock units of the form: \\ \centerline{$\begin{array}{rcll}
x_{n_0}(t) &=& \sum_{n = 0}^{T-1} \bar{o}_{n_0}(n+1) \, H(x_{N_0+n}(t) - 1) & 0 \leq n_0 < N_0 \\
x_{N_0}(t) &=&  W_0 \, x_{N_0}(t-1) + W_2 \, (1 - H(x_{N_0}(t)) \\
x_{N_0+n}(t) &=& x_{N_0+n-1}(t-1) & 0 < n < T \\
\end{array}$} considering $H(0) = 1/2$ by prolongation. As soon as $W_2 > 2, W_0 < 2 / W_2$ we easily obtain: 
\\ \centerline{${\bf x}_{N_0} = \{0,  W_2 / 2 > 1, \cdots, (W_0)^{t-1} \, W_2 / 2 < 1, \cdots\}$} \\
thus $H(x_{N_0}(t) - 1) = \delta_{t=1}$ and $H(x_{N_0+n}(t) - 1) = \delta_{t=n-1}$. In words, we generate $T$ clock signals with one non-zero value at time $n=t+1$, allowing to generate the desired sequence combining these signals. The unit of index $N_0$ is a recurrent unit generating a trigger signal, all other units being feed-forward units. 
If we now consider not Heaviside profiles, but sigmoid $h(u) = \left(1 + e^{-4 * u}\right)^{-1}$, the previous system of equation is going to generate a temporal partition of unity. More precisely we obtain, with say $W_2 = 2$ and $W_0 = 1/4$:
\\ \centerline{${\bf x}_{N_0} = \{0, 1, u_t < 1, \cdots\}$}\\ while numerically $u_t \rightarrow u_{\infty} \simeq 0.419$, the equation fixed point. Since ${\bf x}_{N_0+n}$ are simple shifts of ${\bf x}_{N_0}$, the clock units obviously span the output signal space and output units can easily linearly adjust there related combination to obtain the desired values.}, 
%h := u -> 1 / (1 + exp(-4 * u)): W2 := 2: W0 := 1/4: x_0 := 0: for t to 1000 do x_1 := evalf(W0 * x_0 + W2 * (1 - h(x_0))): print(x_1, x_0-x_1): x_0 := x_1: od:
and no more that $N_0+1$ units if the numerical precision is sufficient\footnote{ {\bf Long sequence generation with a range unit.} We simply consider units of the form: \\ \centerline{$\begin{array}{rcrlllr}
x_{n_0}(t) &=& \sum_{n = 0}^{T-1} \bar{o}_{n_0}(n) \, &(H(x_{N_0}(t) - (n-1)) &\\&&&- H(x_{N_0}(t) - n)) & = \{\cdots, \bar{o}_{n_0}(t), \cdots\} & 0 \leq n_0 < N_0 \\
x_{N_0}(t) &=&  x_0(t-1) + 1 && = \{\cdots, t, \cdots\} \\
\end{array}$}\\ where $H(x_{N_0}(t) - (n-1)) - H(x_{N_0}(t) - n) = \delta_{n=t}$.}. In both cases, the weight estimation reduces to a simple readout estimation (i.e., the linear estimation of output weights, given the predefined hidden unit activity).
A step further, considering $N = O(\sqrt{N_0\,T/R})$ linear or NLN units of range $R$, as developed in \cite{rostro-gonzalez-cessac-etal:10} in a special case, we may generate a solution in the general case\footnote{ {\bf Long sequence generation with fully connected network.} Considering the linear network system:
\\\centerline{$x_n(t) = \sum_{m = 1}^{m  = N} W_{nmr} \sum_{r= 1}^{r=R} \, x_m(t-r) + W_{n0}$,}\\
with $0 \leq n_0 < N_0$ output units and $N_0 \leq n < N$ hidden units, using vectorial notations, with the shift operator ${\cal S}$ defined as ${\cal S} {\bf x}(t-1) = {\bf x}(t)$, we obtain:
\\ \centerline{${\cal S} \, \begin{array}{r}{}_{N_0\,T}\updownarrow\\ {}_{(N-N_0)\,T}\updownarrow \end{array}
\left(\begin{array}{c} \bar{\bf o} \\ \bar{\bf x} \end{array}\right) = {\bf W} \, \left(\begin{array}{c} \bar{\bf o} \\ \bar{\bf x} \end{array}\right) + W_{0}$}\\ where $\bar{\bf o}$ are the desired output. It is a bi-linear system of $N\,T$ equations in $N^2\,R + N$ independent unknowns, i.e., the weights, while the $(N-N_0)\,T$ hidden values are entirely specified as soon as the weights are given. In terms of number of degree of freedom we must have $N^2\,R + N > N_0 \,T$ for this algebraic system of equation to have a solution in the general case.}. The number of required unit can not be higher than $N_0 + T$ as discussed previously.

The generation of periodic signal of period $T$, is a very similar problem, as studied in \cite{rostro-gonzalez-cessac-etal:10}, for $N=N_0$. In a nutshell, we simply must add equations such that ${\bf x}(T) = {\bf x}(0)$ to guaranty the periodicity.

From this discussion, we would like to point out several remarks:
\\ - the complexity of signal generation problem highly depends on the kind of ``allowed units'' and reduces to a trivial problem as soon as clock-like signals are used;
\\ - there always exist a $R=1$ network of at most $N_0 + T$ units that exactly solves the problem, so that this is still an reverse engineering problem;
\\ - a linear network, NLN network or AIF network can generate such a sequence in the general case;
\\ - the use of BMS (a.k.o. spiking neurons) ease the problem statement but does not change qualitatively the solution.

\subsubsection*{Long term non-linear transform}

In many experiment a sequence of the form:
\\\centerline{\begin{tabular}{lcccccccccccc}
time :  & 0   & 1   &     &          &     & T\\
input:  & $a$ & $b$ & $*$ & $\cdots$ & $*$ & $*$\\
output: & $*$ & $*$ & $*$ & $\cdots$ & $*$ & $a \, b$ \\
\end{tabular}}\\
where $a$ and $b$ are variable input, $*$ are random distractors and $a\,b$ the desired delayed output (here a product, but it could be another calculation such as a XOR\footnote{ {\bf Defining the xor function.} Using the same method as before, with $W_1 > 1$: \\ \centerline{$\begin{array}{rcl}
x_\bullet(t) &=& W_1 \, H(x_a(t - 1) - 1) + W_1 \, H(x_b(t - 1) - 1) + -2 \, W_1 \, H(x_o(t) - 1) \\
x_o(t) &=& x_a(t - 1) + x_b(t - 1) - 1 \\
\end{array}$} allows us to obtain $H(x_\bullet(t) - 1) = H(x_a(t - 1) - 1) \mbox{ xor } H(x_b(t - 1) - 1)$.}). Such setup combines several non-trivial aspects, long short term memory, distractor robustness, and operation which may not explicitly hardwired in the network, presently a product. The LSTM approach was shown to be particularly efficient for such computation, because of the notion of ``memory carousel''. The explicit implementation of such a mechanism is however elementary\footnote{ {\bf Long term computation.} One solution writes: \\ \centerline{$\begin{array}{rcl}
o_0(t) &=& H(c_T(t) - 1) \, x_a(t) \, x_b(t) + H(1 - c_T(t)) \, i(t) \\
x_a(t) &=& H(1 - c_0(t)) \, x_a(t-1) + H(c_0(t) - 1) \, i(t) \\
x_b(t) &=& H(1 - c_1(t)) \, x_b(t-1) + H(c_1(t) - 1) \, i(t) \\
c_\tau(t) &=& W_0 \, c_\tau(t-1) + (\frac{1}{\tau + 1/2} + (1 - W_0) \, c_\tau(t-1)) \, (1 - H(c_\tau(t))) \\
\end{array}$} \\ for some value $W_0 < 3/4$. It is easy to verify that:
\\ 1/ ${\bf c}_\tau = \{0, \underbrace{t / (\tau + 1/2)}_{t < \tau},  \underbrace{(\tau+1) / (\tau+1/2)}_{t = \tau}, \underbrace{W_0^{t-\tau} \, (\tau+1) / (\tau+1/2)}_{\tau < t}\}$ so that $H(c_\tau(t) - 1) = \delta_{t=\tau}$ is a clock signal. 
\\ 2/ $x_a(t)$ ``opens'' the memory at time $t=0$, and stores the previous value otherwise, since $H(1 - z) = (1 - H(z - 1))$ is simply a ``negation'', with a similar behavior for $x_b(t)$.
\\ 3/ $o_0(t)$ simply mirror the input until $t=T$, where the expected result is output.} while several variants of long-term mechanism can easily be defined, such a flip-flop\footnote{ {\bf Defining a flip-flop latch.} Let us defined a SR-latch (i.e., a flip-flop) with:
\\ \centerline{$\begin{array}{rcl}
z(t) &=& W_1 \, H(z(t-1) - 1) + W_1 \, H(i_1(t) - 1) - W_1 \, H(i_0(t) - 1)\\
\end{array}$} \\ 
with $1 < W_1$, considering the binary signal $H(z(t-1) - 1)$ and yielding the following behavior:
\\- {\em R-state}: If $i_0(t) < 1$ and $i_1(t) < 1$ (no-input) and $z(t-1) < 1$, then $z(t) = 0 < 1$, the reset state is maintained.
\\- {\em S-state}: If $i_0(t) < 1$ and $i_1(t) < 1$ (no-input) and $z(t-1) > 1$, then $z(t) = W_1 > 1$, the set state is maintained.
\\- {\em R-S transition}: If $i_0(t) < 1$ and $i_1(t) > 1$  and $z(t-1) < 1$, then $z(t) = W_1 > 1$, flipping to a set state; if it was already in the set state, we still have $z(t) = 2 \, W_1 > 1$.
\\- {\em S-R transition}:If $i_0(t) > 1$ and $i_1(t) < 1$  and $z(t-1) > 1$, then $z(t) = W_1 - W_1 < 1$, flopping to a reset state; f it was already in a reset state, we still have $z(t) = - W_1 < 1$.
\\- {\em no instability}: $i_0(t) > 1$ and $i_1(t) > 1$ contrary to a standard digital RS-latch we simply have $z(t) = z(t-1)$ providing it was in set of reset state, without any meta-stability.
If now we consider a sigmoid instead of a step function, [A COMPLETER]
}
