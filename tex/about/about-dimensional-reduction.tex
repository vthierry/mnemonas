\documentclass{article}\usepackage{hyperref}\newcommand{\deq}{\stackrel {\rm def}{=}} \newcommand{\eqline}[1]{\\\centerline{$#1$}\\}\newcommand{\tab}{\hphantom{6mm}}\newcommand{\well}[1]{\vspace{0.3cm}\hspace{-2cm}{\tt #1}} \begin{document}

problem of dimensionality: A simple example \cite{trunk1979problem}

Dimensionality reduction a short tutorial \cite{ghodsi2006dimensionality}.

Dimensionality reduction: a comparative review \cite{van2009dimensionality}.


fractional distance metric can significantly improve the effectiveness of standard clustering algorithms such as the k-means
algorithm in huge dimension \cite{aggarwal2001surprising}

Laplacian eigenmaps for dimensionality reduction and data representation \cite{belkin2003laplacian}

a need to integrate dimensionality reduction techniques with data mining approaches and graph theory \cite{pandove2017local}, hierarchical clustering.

linear manifold clustering, i.e., identifies subsets of the data which are embedded in arbitrary oriented lower dimensional linear manifolds, improves stability, accuracy, and computation time, if adapated to the data \cite{haralick2005linear,haralick2007linear}.

Certain types of high bias can be canceled by low variance to produce accurate classification \cite{friedman1997bias}.

sparse estimation tasks can be performed efficiently in high dimensions, in the robust setting where an Îµ-fraction of samples are corrupted adversarially \cite{li2017robust}

outlier detection on high dimensional data using reverse nearest neighbor \cite{rohini2017outlier} or using k-nearest neighbour graph \cite{hautamaki2004outlier}. A survey on unsupervised outlier detection in high-dimensional numerical data \cite{zimek2012survey}.

manifold learning preserving the geometry of a data set, by augmenting the output of embedding algorithms with geometric information embodied in the Riemannian metric of the manifold \cite{perraultmetric}

\bibliographystyle{alpha}\bibliography{../etc/main.bib} \end{document}


