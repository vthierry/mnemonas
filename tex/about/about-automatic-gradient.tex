\documentclass{article}\usepackage{hyperref}\usepackage{graphicx}\usepackage{amsmath}\usepackage{amsfonts}\newcommand{\deq}{\stackrel {\rm def}{=}} \newcommand{\eqline}[1]{\\\centerline{$#1$}\\}\newcommand{\tab}{\hphantom{6mm}}\newcommand{\well}[1]{\vspace{0.3cm}\hspace{-2cm}{\tt #1}} \begin{document}

\section*{Introduction}

\subsection*{Problem position}

Automated machine learning\footnote{The notions of ``\href{https://en.wikipedia.org/wiki/Automated\_machine\_learning}{automated-machine-learning}'', ``\href{https://en.wikipedia.org/wiki/Meta_learning_(computer_science)}{meta-learning}'', including ``\href{https://en.wikipedia.org/wiki/Meta-optimization}{meta-optimization}'' and ``\href{https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)}{hyperparameter}'' and the related ``\href{http://neupy.com/2016/12/17/hyperparameter\_optimization\_for\_neural\_networks.html}{hyperparameter optimization}'', have precise meaning in machine-learning, and we assume this is known to the reader.} is ``the process of automating the end-to-end process of machine learning'', because the complexity of adjusting the hyperparameters, including selecting a suitable method, becomes easily time-consuming when not intractable. To this end, the general idea is to consider a standard machine-learning algorithm and to add on ``top of it'' another algorithmic mechanism, e.g., another machine learning algorithm dedicated to automatic hyperparameter adjustment, with the caveat of generating other hyperparameters for the meta-learning algorithm and without formal guaranty that this accumulation of mechanisms is optimal.

The key setting is to adjust the model parameter by optimizing the criterion on a {\em learning} data set, and validate the hyperparameter choice by rerunning the optimization for different hyperparameter values on a different {\em validation} data set, while the final performance is evaluated on another {\em test} set. 

\subsection*{Related work}

As being an everyday concrete problem for the machine learning community, there is a huge literature on hyperparameter adjustment of optimization algorithms. A recent review of automatic selection methods for machine learning algorithms and hyperparameter values is available \cite{luo2016review}, while the state of the art regarding hyper-heuristics has been made by \cite{burke2013hyper}, an introducing overview about automatic hyperparameter optimization and model selection for supervised machine learning is available in this student work \cite{bermudez2014automatic}. Hyperparameter adjustment packages are available with every machine learning framework, searching through the joint space of hyperparameter settings such as, e.g., Hyperopt \cite{bergstra2013hyperopt} or, e.g., Auto-WEKA \cite{kotthoff2017auto}. Further reviewing these methods is beyond the scope of this paper.

As far as gradient adjustment is concerned, adaptive learning rate methods has been explored, e.g., \cite{zeiler2012adadelta} for the ADADELTA method, or other alternate methods \cite{tseng1998incremental}. One track is to consider algorithms for which hyper-parameter tuning requires little tuning (e.g., considering the ADAM approach of \cite{kingma2014adam}, where the first-order gradient-based optimization of stochastic objective functions is based on adaptive estimates of lower-order momenta, or improving vanilla or natural gradient methods by automatic adjustment of the local optimization \cite{marceau2016practical}). A step further, gradient has also been adjusted adjustment has been considered using a sub-set of the network parameters, considering that the intrinsic dimension of the objective landscape is below the global number of parameters \cite{li2018measuring}. 

Gradient descent optimization algorithms has been reviewed in \cite{ruder2016overview}, and stochastic gradient further discussed in \cite{bottou2012stochastic}, while the link between momentum methods and stochastic gradient is discussed in \cite{loizou2017momentum}. Further improvements has been proposed, e.g., by \cite{konevcny2017semi} considering an improved random selection of some hyper-parameters, \cite{le2012stochastic} taking a memory of previous gradient values into account, or \cite{johnson2013accelerating} by implicitly combining stochastic gradient with momentum like methods to reduce variance. The momentum mechanism, though sensitive to initial conditions (see, e.g., \cite{sutskever2013importance}), is very interesting because it is in deep relation with reduced forms of 2nd order methods \cite{bhaya2004steepest}, while such methods do not indeed have the burden of adjusting the learning rate, and better convergence performances.

A step further, the link between stochastic gradient descent and evolutionary algorithms has been made, e.g., in \cite{meuleau2002ant} considering ant inspired algorithms. Another track, is to design a meta-learning mechanism in the sense of having an algorithm learning to tune the gradient descent as in \cite{andrychowicz2016learning}. Though both tracks are interesting, our main positioning here is to consider automating gradient descent {\em within} the gradient descent algorithm.

\subsection*{Proposed contribution}

In all approaches reviewed previously there are hyper-hyper-parameters. Methods like ADAM still maintain a ``step-size'' in gradient descent, and even methods like ADADELTA that get rid of the learning rate hyper-parameter, introduce new hyper-parameters, mainly filtering windows to obtain averaged value of gradient or step momenta, except in \cite{schaul2013no} where this parameter is automatically adjusted by heuristic (which rationale is not detailed). Moreover, in all methods ``epsilon'' values are required because a strictly positive ratio is computed which denominator and numerator can not vanish, they thus must be higher than some ``epsilon''. Furthermore, during the very first steps, the implicit model is not yet estimated and the mechanism is not valid. A step further, in all methods a design choice has to be made between adjusting globally the gradient learning rate, or considering a local adjustment on each component. The latter choice has the advantage of allowing 2nd order minimization approximation, but the drawback that the result of this adaptation is perturbed by other components adjustments, while the choice of adjusting each component one after another yields a very large number of criterion estimation.

 Considering these issues we are going to revisit these methods and propose a variant for which neither the starting phase nor the quantity vanishing has to be controlled by hyper-parameters.

Another aspect concerns the data use choices: One minimization could be done by selecting only one sample after another in a random manner (thus shuffling the samples, this method is called stochastic gradient) or a mini-batch of samples (but of which size ?), or the complete batch of samples, i.e. the whole epoch. The number of iterations for a given mini-batch, and for the whole epoch is also to be adjusted. And minimizing on a learning set is not sufficient, we must also validate this estimation on a test set, thus split (but to which amount ?) the epoch in these two subsets. The adjustment is made by minimizing a continuous loss on the learning set, but the final performance estimate is more an accuracy, i.e., in classification task the proportion of test samples well classified, which is not a continuous measure.

 In this work we are not going to address these issues but use a fix paradigm. We consider only selecting only one sample after another, because we consider that the mini-batch smoothing effect is compensated by temporarily smoothing the different quantities in the algorithm. We fix the learning/validation sample ration to 80\%/20\% following a standard choice. We run several epochs with an automatic detection of convergence, detailed in the sequel.

\section*{Problem setting}

\subsection*{Notations}

To develop our idea we are going to consider a simple input-output transformation :
\eqline{{\bf o} = {\bf f}_{\bf w}({\bf i}),}
mapping an input ${\bf i} \in {\cal R}^I$ onto an output ${\bf o} \in {\cal R}^O$, as a function of parameters (e.g., network weights or architecture size) ${\bf w} \in {\cal M} \subset {\cal R}^W$, and a sequence of input ${\bf i}_n, t \in \{1, N\}$, coupled with a output loss function sequence $l_{n, {\bf w}}(\cdot) > 0$ with the goal to minimize the loss expectation: 
\eqline{{\bf w}^\bullet = \mbox{arg max}_{\bf w} \mathbb{E}[l_{\bf w}({\bf f}_{\bf w}({\bf i}))]}
and would like, given a data set, to minimize this expectation on subsequent input.

%typos sur l_

The function parameters ${\bf w}$ lives in a compact (thus bounded) subset ${\cal M} \subset {\cal R}^W$. This means concretely two things : We have an estimation of the weights minimal and maximal values, and not all the weights adjustment is significant as studied in \cite{li2018measuring}.

We consider that the criterion function $l_{\bf w} \deq l_{\bf w}({\bf f}_{\bf w}({\bf i}))$ and its gradient vector ${\bf g}_{\bf w} \deq \nabla_{\bf w} l_{\bf w}({\bf f}_{\bf w}({\bf i}))$ is calculated analytically. 

We can not always consider the case where the diagonal vector ${\bf h}_{\bf w}$ of the Hessian $h_{{\bf w} j} \deq \nabla^2_{w_j w_j} l_{\bf w}({\bf f}_{\bf w}({\bf i}))$ can be provided by analytical calculations, through this is expected to improve performances, while the derivation is quite easy for usual neural network models\footnote{{\bf On non-linear units 2nd order derivatives} Consider a standard non-linear unit of the form: 
\eqline{o = f_{\bf w}({\bf i}) \deq \sigma(\sum_j w_j \, i_j), \mbox{ with } \nabla_{\bf w} o = \sigma'(\sum_j w_j \, i_j) \, {\bf i}, \nabla^2_{{\bf w},{\bf w}} o = \sigma''(\sum_j w_j \, i_j) \, {\bf i} \, {\bf i}^T.}
If the non-linear activation function is a sigmoid we obtain 2nd order function which is well defined and which values vanishes with the unit saturation. If the  non-linear activation function is ReLU i.e. a rectification, then the 2nd order derivation vanishes. This means that analytically calculated 2nd order derivatives are not usable in this case.}.We thus are going to consider that ${\bf h}$ is observable in the sequel, while introducing this source of information if available is a straightforward improvement of the method.

Following the reviewed methods we are going to consider a diagonal 2nd order model, and the related parameter increments:
\begin{equation} \label{2nd-diagonal-model} \begin{array} {rcl}
 l_{{\bf w} + \delta_{\bf w}} &=& l_{\bf w} + {\bf g}_{\bf w}^T \, \delta_{\bf w} + \delta_{\bf w}^T \, diag({\bf h}_{\bf w}) \, \delta_{\bf w} / 2 + {\bf \nu}_l \\
  {\bf g}_{{\bf w} + \delta_{\bf w}} &=& {\bf g}_{\bf w} + {\bf h}_{\bf w} \odot \delta_{\bf w} + {\bf \nu}_g \\
  {\bf h}_{{\bf w} + \delta_{\bf w}} &=& {\bf h}_{\bf w} + {\bf \nu}_h \\
  0 &=& {\bf g}_{\bf w} + {\bf h}_{\bf w} \odot \delta_{\bf w}^\bullet \\
\end{array}\end{equation}
where $\delta_{\bf w}^\bullet$ is the parameter increment corresponding to a one-step optimal minimization, and ${\bf \nu}$ is a zero-mean random noise that encounter for the fact that the estimation is both noisy and not stationnary (which is viewed as the fact the previous estimations were noisy).

\section*{Proposed method}

\subsection*{Initial steps}

At the very beginning of the estimation, we only have one estimation of the criterion to minimize and the related gradient, and from~(\ref{2nd-diagonal-model}) neglecting the 2nd order term and the noise, thus implicitly assuming the criterion is linear obtain a 1st trivial minimal magnitude estimate to obtain $l_{{\bf w} + \delta_{\bf w}} = 0$, i.e.:
\eqline{\delta_{\bf w}^0 = {\bf g}_{\bf w} / l_{\bf w}}
which is well defined as soon as $l_{\bf w}$ which is strictly positive does not vanish (otherwise this means that we already are at the optimum). Here, we implicitly assume that at the optimum $l_{{\bf w}^*} \ll l_{\bf w}$, which is reasonable at the beginning of the minimization, but not close to the optimum.

As a consequence, we do not expect this to be very efficient: This will very likely generates an increment which is too high. But it is very easy to to adjust.
To this end, we propose to start with the simple heuristic: {\tt 
\\ - Compute $\delta_{\bf w}^0$ and set $\lambda = 1$
\\ - Repeat :
\\ \tab - Set $\lambda \leftarrow \lambda/2$, and calculate $l_\lambda \deq l_{{\bf w} + \lambda \, \delta_{\bf w}^0}$
\\ - until $l_\lambda < l_0$
\\ - Set $\lambda \leftarrow 3/2\,\lambda$, }

We know that this will converge, as soon as the gradient is well defined. This will allow us to find a initial step, but also to start estimating the model in~(\ref{2nd-diagonal-model}), as detailed in the next subsection.

With this design choice, the initial increment order of magnitude is large. This may be a problem if the goal is to look for a local minimum, i.e., the closest criterion local minimum with respect to the initial parameter value, but this is a rare requirement. In almost every case, being able to ``jump'' far from the initial estimate is a desired property.

Repeating this heuristic provides, as it, a hyperparameter-less 1st order minimization method. The normalized learning rate $\lambda = \frac{3^p}{2^{p+q}}$, where $p$ is the number of successful minimization and $q$ the number of failed minimization, automatically decrease or increase to maximize the criterion minimization at each step. The choices of $1/2$ and $3/2$ are totally arbitrary: We have simply chosen the simplest decrease factor, of $2$, i.e. of one octave or one bit and have chosen the simplest augmentation factor below $2$. We could also have chosen a random factor. To get a step further we must consider 

\subsection*{Local model adjustment}

Contrary to usual methods based on the derivation of an update rule obtained from an analysis of a model corresponding or close to~(\ref{2nd-diagonal-model}), here we address the explicit adjustment of the model parameters. At each step we obtain an estimation of $l_{\bf w}$, ${\bf g}_{\bf w}$ from the criterion and given, and know the last $\delta_{\bf w}$ value. We thus can obtain an estimate of ${\bf h}_{\bf w}$ in the weighted least-square sense:
\eqline{\begin{array}{ll} 
 {\bf h}_{\bf w}^N = \mbox{arg min}_{{\bf h}_{\bf w}} & \;
  \frac{1}{\sigma_{{\bf \nu}_l}^2} \, \left(l_{{\bf w} + \delta_{\bf w}} - \left(l_{\bf w} + {\bf g}_{\bf w}^T \, \delta_{\bf w} + \delta_{\bf w}^T \, diag({\bf h}_{\bf w}) \, \delta_{\bf w} / 2\right)\right)^2 \\  & +
  \frac{1}{\sigma_{{\bf \nu}_g}^2} \, \left({\bf g}_{{\bf w} + \delta_{\bf w}} - \left({\bf g}_{\bf w} + {\bf h}_{\bf w} \odot \delta_{\bf w}\right)\right)^2 
\end{array}}
yielding a linear equation in ${\bf h}_{\bf w}$.

At step $N$, we can estimate the ${\bf \nu}$ variance from:
\eqline{\begin{array}{rcl}
\sigma_{{\bf \nu}_l}^2 &\leftarrow& \frac{N-1}{N} \, \sigma_{{\bf \nu}_l}^2 + \frac{1}{N} \, \left(l_{{\bf w} + \delta_{\bf w}} - \left(l_{\bf w} + {\bf g}_{\bf w}^T \, \delta_{\bf w} + \delta_{\bf w}^T \, diag({\bf h}_{\bf w}^N) \, \delta_{\bf w} / 2\right)\right)^2 \\
\sigma_{{\bf \nu}_{\bf g}}^2 &\leftarrow& \frac{N-1}{N} \, \sigma_{{\bf \nu}_{\bf g}}^2 + \frac{1}{N} \, \left({\bf g}_{{\bf w} + \delta_{\bf w}} - \left({\bf g}_{\bf w} + {\bf h}_{\bf w}^N \odot \delta_{\bf w}\right)\right)^2 \\
%\sigma_{{\bf \nu}_{\bf h}}^2 &\leftarrow& \frac{N-1}{N} \, \sigma_{{\bf \nu}_{\bf h}}^2 + \frac{1}{N} \, \left({\bf h}_{\bf w}^N - {\bf h}_{\bf w}^{N-1}\right)^2 \\
\end{array}}



{\small \bibliographystyle{alpha}\bibliography{../bib/vthierry}} \end{document}
