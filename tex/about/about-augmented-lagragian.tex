\documentclass{article}\usepackage{hyperref}\newcommand{\deq}{\stackrel {\rm def}{=}} \newcommand{\eqline}[1]{\\\centerline{$#1$}\\}\newcommand{\tab}{\hphantom{6mm}}\begin{document}

\section*{Minimizing a huge criterion, under constraints}

\subsection*{Problem position}

We consider minimizing the loss function $l({\bf x})$, $l: {\cal R}^D \rightarrow {\cal R}^+$ (the ideal loss being $0$), under the constraints
\eqline{{\bf x} \in {\cal C} = \{{\bf x}, {\bf c}({\bf x}) = 0\}, \; {\bf c}: {\cal R}^K \rightarrow {\cal R},} 
considering the \href{https://en.wikipedia.org/wiki/Augmented_Lagrangian_method}{augmented Lagrangian's} to minimize:
\[
{\cal L}_{{\bf \lambda}, {\bf \mu}}({\bf x})= \gamma \, l({\bf x}) + \sum _{k} {\frac{\mu_{k}}{2}}\,c_k({\bf x})^{2}+\sum _{k} \, \lambda _{k}\, c_{k}({\bf x})
\]
where ${\bf \mu} = \{\cdots \mu_k  \cdots\}$ are penalty terms and ${\bf \lambda} = \{\cdots \lambda_k  \cdots\}$ are Lagrangian multipliers, while $\gamma \in \{0, 1\}$ is a simple notation.

- The key point here is that $D$ and $K$ are large so that 2nd order methods are not tractable, but only a 1st order local minimization algorithm ${\cal A}$ is available:
\eqline{{\bf x}_{\gamma, {\bf \mu}, {\bf \lambda}, {\bf x}_\bullet} \deq \mbox{arg min} {\cal L}_{\gamma, {\bf \mu}, {\bf \lambda}}({\bf x}), \mbox{starting from }{\bf x}_\bullet}
and we assume that such and algorithm exists, while the loss gradient $\partial_{\bf x} l({\bf x})$ and constraint Jacobian $\partial_{\bf x} {\bf c}({\bf x})$ are available.

- With respect to the vanilla Augmented Lagrangian method (with all $\mu_k$ equal to a unique $\mu$), $\mu$ being an algorithmic hyper-parameter, here, we propose a generic hyper-parameter-less mechanism, contrary to usual Augmented Lagrangian methods \cite{lewis2002globally,luo2007convergence} or Alternating Direction Method of Multipliers methods \cite{boyd2011distributed}. A necessary and sufficient condition, reused here, for the exact penalty representation in the framework of a generalized augmented Lagrangian is by \cite{huang2003unified}.

\subsection*{Meta-parameters specification}

We consider having an initial estimate, i.e. default value or starting point ${\bf x}_0$, with initial loss, and constraint initial bias:
\eqline{l_0 \deq l({\bf x}_0) \mbox{ and } \beta_0 \deq \|{\bf c}({\bf x}_0)\| = \sqrt{{\cal L}_{0, 2, 0}({\bf x}_0)}.}

We start calculating the unconstrained minimum ${\bf x}_{l_{\min}} \deq {\bf x}_{1, 0, 0, {\bf x}_0}$ and related loss $l_{\min} \deq l({\bf x}_{l_{\min}})$, and $\beta_{\max} \deq \|{\bf c}({\bf x}_{l_{\min}})\|$ for two reasons: First, we want to have a lower-bound of the criterion (obviously the unconstrained minimum cannot be worst than the constrained one), and upper-bound of the bias (obviously the unconstrained minimum does not consider the constraint and any further optimization must improve it). Moreover, this means looking for a minimum in ${\bf R}^D$ and there is more chance for the problem to be locally convex in the ``big'' space than on the manifold ${\cal C}$. We consider this general design choice for an initial estimate better than minimizing after constraining the estimate to be on the manifold.

We then calculate the projection onto ${\cal C}$, from both the initial state and the unconstrained minimum:
\eqline{{\bf x}_{l_{\max},0} \deq {\bf x}_{0, 1, 0, {\bf x}_{0}} \mbox{ and } {\bf x}_{l_{\max},1} \deq {\bf x}_{0, 1, 0, {\bf x}_{l_{\min}}}}
and consider the related loss upper-bound $l_{\max} \deq \max(l_{{\max},0}, l_{{\max},1})$ and related bias lower-bound 
$\beta_{\min} \deq \min(\beta_{{\min},0}, \beta_{{\min},1})$. Obviously, minimizing the loss under the constraint must improve this value, while the constraint bias cannot be lower if the loss is taken into account.

\begin{tabular}{l|l|l|l|}
 & State & Loss value & Constraint bias \\
\hline
Initial estimate & ${\bf x}_0$ & $l_{0}$ & $\beta_0$ \\
Unconstrained minimum & ${\bf x}_{l_{\min}}$ & $l_{\min}$ & $\beta_{\max}$ \\
\parbox{4cm}{Projection onto the constraint, here $u\in\{0,1\}$} & ${\bf x}_{l_{\max},u}$ & $l_{{\max},u}$ & $\beta_{{\min},u}$ \\
\hline
\end{tabular}

~\\

A step further,consider having an a-priory information on either the loss function precision $\epsilon_l$ or the state precision ${\bf \epsilon}_{\bf x}$, or the constraint precision ${\bf \epsilon}_{\bf c}$. By precision we mean that two values which distance is below the precision are indistinguishable. We also consider knowing a reasonable bound of usual computations precision $\epsilon_\bullet$, i.e., $\epsilon_\bullet \simeq 10^{-12}$ for usual complex non-linear double precision computations. All precision are positive and likely higher than $\epsilon_\bullet$. Since we have gradient and Jacobian, we do not need all this information, but only one, since they are easily related.
\eqline{\begin{array}{lll}
 \epsilon_l \simeq |\partial_{\bf x} l({\bf x}) \, {\bf \epsilon}_{\bf x}|, &
 {\bf \epsilon}_{{\bf c}_k} \simeq |\partial_{\bf x} {\bf c}_k({\bf x}) \, {\bf \epsilon}_{\bf x}|, & \mbox{while} \\
 {\bf \epsilon}_{{\bf x}_i} \simeq \frac{\epsilon_l}{\epsilon_\bullet + |\partial_{{\bf x}_i} l({\bf x})|}, &
 {\bf \epsilon}_{{\bf x}_i} \simeq \min_k\, \frac{\epsilon_{c_k}}{\epsilon_\bullet + |\partial_{{\bf x}_i} c_k({\bf x})|}, & \mbox{and} \\
 \multicolumn{2}{l}{{\epsilon}_{\cal L} \simeq \gamma \, \epsilon_l + \sum_k \frac{\mu_k}{2} \, {\bf \epsilon}_{{\bf c}_k}^2 + \lambda_k \, {\bf \epsilon}_{{\bf c}_k}.}
\end{array}}
These order of magnitudes computed related to $l(\cdot)$ are to be done ${\bf x}_0$ since $\|\partial_{\bf x} l({\bf x})\|$ is expected to vanish at the minimum. Those in relation with ${\bf c}(\dot)$ may be recomputed during the estimation process.

Finally, we consider that at each minimization step, the mechanism of gradient descent can ``hook'' a piece of code that detects if convergence succeeds or fails and optionnaly adjust the internal parameters. This may write:
{\tt ~
\\\indent ${\cal A}_{\mbox{initialize}}({\cal L}, {\bf x} = {\bf x}_0)$
\\\indent do \{
\\\indent \indent ${\cal A}_{\mbox{run-one-step}}({\cal L}, {\bf x})$
\\\indent \indent ${\cal A}_{\mbox{adjust meta-parameters}}()$
\\\indent \} until(${\cal A}_{\mbox{has-converged-or-it-is-too-late}}()$)
\\
}
without further consideration about the maximal number of iterations (or maximal computation time), considered here as an application dependent parameter.

Thanks to these design choices, providing only the initial state value ${\bf x}_0$ and one indication of precision, let us revisit the augmented lagrangian method.

\section*{Internal parameter adjustment}

Given the previous meta-parameters, we can specify several aspects of the computation:
\\- The problem is solved the related bias is below the precision on the constraint, and the loss cannot be further decreased.
\\- One minimization has converged for a given set of internal parameters as soon as from one iteration to another either the state variation is below ${\bf \epsilon}_{\bf x}$ or the criterion value do not decrease more than $\epsilon_{\cal L}$. 

The standard Augmented Lagragian method states that when one minimization has converged we must update ${\bf \lambda}$ using the rule 
\[
 \lambda_k^{t+1} \leftarrow  \lambda_k^{t} + \mu_k^{t} \, c_k({\bf x})
\]
at the last optimizes state value ${\bf x}$, allowing to update the Lagrangian multiplier, and it is known (e.g., \cite{}) that under reasonnable assumptions this yields linear convergence.

Here we would like to improve the choice of $\mu_k$ in order to guaranty that $|c_k({\bf x})|$ decreases during the next iteration. 

%Here we introduce another heuristic: Let us tune $\mu_k$  At the 1st order, this means that we need to maintain at the beginning of iteration $t+1$,  \eqline{0 < \partial_{\bf x} {\cal L}({\bf x})^{t+1} \cdot \partial_{\bf x} c_k({\bf x}) \, c_k({\bf x})}, i.e.: \eqline{\|\partial_{\bf x} c_k({\bf x})\|^2 \, |c_k({\bf x})| \, \mu_k^{t+1} + (\partial_{\bf x} {\cal L}({\bf x})^{t} \cdot \partial_{\bf x} c_k({\bf x})) \, sg(c_k({\bf x}))> 0,} as obtained from a few algebra considering the $\lambda_k^{t+1}$ update rule and all other quantities as fixed.

\section*{Hyper-parameter less gradient descent}

When the state dimension $D$ is large considering methods with memory consumption more than $O(D)$ (thus not $O(D^2)$) or computation complexity more than $O(D)$ is untractable, so that 2nd order methods are excluded. This does not however means that only the vanilla 1st order gradient descent method is available, while several improvement has been considered (à détailler).

Here we would like to propose an integrated method that automatically choose and adapt such methods to the ongoing optimizaton process. The key idea is based on the knowledge of the different meta-parameters order of magnitude. 

Let us consider a rule of the form:
\eqline{{\bf x}^{t+1}_\rho \deq {\bf x}^{t} - \rho \, {\bf g}({\bf x}),}
where, we may consider ${\bf g}({\bf x}) = \partial_{\bf x} {\cal L}({\bf x})^T$ or another descent direction. 

At the 1st order:
\eqline{\begin{array}{rcl} {\cal L}({\bf x}^{t+1}) 
&\simeq& {\cal L}({\bf x}^t) + \partial_{\bf x} {\cal L}({\bf x}) \, ({\bf x}^{t+1} - {\bf x}^{t}) \\
&=& {\cal L}({\bf x}^t) - \rho \, \partial_{\bf x} {\cal L}({\bf x}) \, {\bf g}({\bf x}) \\
\end{array}}
allowing us to state: $\rho_{\max} \deq \frac{{\cal L}({\bf x}^t)}{\partial_{\bf x} {\cal L}({\bf x}) \, {\bf g}({\bf x})}$, which corresponds, in a perfect linear case, to obtain the best loss zero value. This is never the case in practice, but it allows to derive a vanilla hyper-parameter less algorithmic step that:
{\tt ~
\\ \indent - Computes the gradient ${\bf g}({\bf x}) = \partial_{\bf x} {\cal L}({\bf x})$ and $\rho_{\max} \deq \frac{{\cal L}({\bf x}^t)}{\epsilon_\bullet + \|{\bf g}({\bf x})\|^2}$ 
\\ \indent - Performs a line search on ${\bf x}^{t+1}_\rho$ for $\rho \in [0, \rho_{\max}]$, 
\\ \indent \indent until the search interval $\delta_{\rho}$ size verifies $\delta_{\rho} \, \|{\bf g}({\bf x})\| < \|\epsilon_{\bf x}\|$
}

Ce n est que le d\'ebut ...



\end{document}


