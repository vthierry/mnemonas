\section{Comparison with related recurrent weight estimation methods} \label{backpropag}

In this section we briefly discuss how this method compares with existing methods of recurrent weight methods estimation.

The back-propagation through time (BPTT) is a gradient-based technique used, .e.g., in Elman's Networks \cite{elman:90}, where the standard back-propagation algorithm is applied to both the network recurrent layers and through time. It is based on the propagation of the error gradient, and it generally remains on two assumptions that the cost is additive with respect to training examples and that it can be written as a function of the network output (see, e.g., \cite{Nielsen2015}). With respect to this basic method, our method:
\\- does not rely on the cost gradient propagation, but the error backward propagation (or tuning), while gradients remain local to a unit.
\\- has been stated including for non additive costs (such as statistical criteria) and for both supervised criterion based on the network output error, or other unsupervised criteria.

Our formulation has been formalized, by, e.g. \cite{cun_theoretical_1988}, but without proposing a second order estimation method, considering explicitly the backward tuning of the error with a heuristic to avoid extinction and explosion. Moreover, the fact this formalism has been applied on the formulation propose in section~\ref{position} with intermediate variables makes the backward tuning proposal more efficient, than if non linearity and weights linear combination have been mixed.

Furthermore, as made explicit in \cite{doi:10.1162/089976603762552988} when comparing back-propagation with contrastive Hebbian learning, or in \cite{cun_theoretical_1988}, our backward tuning mechanism corresponds gradient back-propagation up to a change of variable. However contrary to \cite{doi:10.1162/089976603762552988} or \cite{Hochreiter:1997}, there is no need to introduce further approximation (such as, e.g, only considering diagonal terms) in order to write the backward propagation rule. This variant is well-founded, simpler to write and seems to be numerically more stable.

A step further, artificial neuron network back-propagation has been related to biological back-propagation in neurons of the mammalian central nervous system (see, e.g., \cite{Stuartetal1997}) and it is clear that the propagation of a learning or adaptive error, is more likely to be related to backward tuning of an error, than an energy or criterion gradient minimization. Regarding biological plausibility, our method only involves local distributed adjustments, as a version of back-propagation that can be computed locally using bi-directional activation recirculation \cite{HintonMcClelland1988} instead of back-propagated error derivatives is more biologically plausible, and has been improved by \cite{OReilly1996recirculation}. In its generalized form it also communicates error signals, being inspired by contrastive learning, and using the Pineda and Almeida algorithm \cite{Pineda1987}. All these methods operate on the current estimate of the derivative of the error, not the backward tuning error defined here, while related to specific cost function.

The proposed method also enjoy an interesting interpretation related to the 2nd order estimation method, as made explicit in footnotes${}^{\ref{improvingstate}}$ and ${}^{\ref{improvingkappa}}$. Thanks to the simple formulation, and either from the backward tuning of the estimation error in the case of footnote${}^{\ref{improvingstate}}$ or by direct estimation in the case of footnote ${}^{\ref{improvingkappa}}$ we obtain an estimation not only of the output desired value, but also of hidden state desired value. This corresponds to a deterministic estimation / minimization algorithmic scheme : estimation of the desired hidden state value, given the current weight values followed by the local minimization of the criterion adjusting the unit weights.

As it, even if in relation with the usual standard back-propagation method, the proposed method is a real alternative.



