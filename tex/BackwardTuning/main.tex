\documentclass[a4,12pt,twoside]{article}
\newif\ifRR
\RRtrue % \RRfalse or \RRtrue

\ifRR
\usepackage[a4paper]{geometry}
\usepackage{RR}
\graphicspath{{../etc/rr-sty/}}
\else
\usepackage[top=1.5cm,right=4cm,bottom=2cm,left=1.5cm]{geometry}
\fi
\usepackage[T1]{fontenc} \usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{graphicx} 
\usepackage{hyperref}
\usepackage{amsmath}\usepackage{amsfonts}
\newcommand{\deq}{\stackrel {\rm def}{=}}
\newcommand{\eqline}[1]{\\\centerline{$#1$}\\} 
\newcommand{\hr}{\\---------------------------------------------------}
\newcommand{\tab}{\\\hspace{5mm}} 

\usepackage{color}
\definecolor{fred}{RGB}{32,64,128}\newcommand{\fred}[1]{{\color{fred}{{\tt @fred:} #1}}}
\definecolor{thalita}{RGB}{51, 153, 255}\newcommand{\thalita}[1]{{\color{thalita}{{\tt @thalita:} #1}}}
\definecolor{vthierry}{RGB}{80,0,120}\newcommand{\vthierry}[1]{{\color{vthierry}{{\tt @vthierry:} #1}}}
\definecolor{xavier}{RGB}{42,120,42}\newcommand{\xavier}[1]{{\color{xavier}{{\tt @xavier:} #1}}}

\ifRR
\RRNo{9100}
\RRdate{October 2017}
\RRauthor{\thanks[affil]{Mnemosyne team, INRIA Bordeaux}
Thierry Vi\'eville \thanksref{affil} \and
Xavier Hinaut \thanksref{affil} \and
Thalita F. Drumond \thanksref{affil} \and
Fr\'ed\'eric Alexandre \thanksref{affil}}
\authorhead{Alexandre \& Drumond \& Hinault \& Vi\'eville}
\RRtitle{Estimation des poids d'un réseau récurrent \\ par ajustement rétroactif}
\RRetitle{Recurrent neural network weight estimation \\ through backward tuning}
\titlehead{Backward tuning}
\RRresume{Nous considérons une formulation alternative de l'estimation du poids dans les réseaux récurrents, proposant une notation integrant une grande quantité d'unités de réseau récurrentes qui aide à formuler ce problème d'estimation. Réutilisant un «bon vieux» principe de la théorie du contrôle, amélioré ici à l'aide d'une heuristique de stabilisation numérique rétroactive, nous obtenons une estimation distribuée du 2ème ordre, numériquement stable et plutôt efficace, sans aucun méta-paramètre à ajuster. La relation avec les techniques existantes est discutée à chaque étape. La méthode proposée est validée en utilisant des tâches d'ingénierie inverse.}
\RRabstract{\input{conclusion}}
\RRmotcle{réseaux récurrents, apprentissage automatique, ajustement rétroactif}
\RRkeyword{recurrent network, machine learning, backward tuning}
\RRprojet{Mnemosyne}
\RCBordeaux
\fi

\begin{document}
\ifRR
\makeRR 
\else
\title{Recurrent neural network weight estimation \\ though backward tuning}
\author[1]{Fr\'ed\'eric Alexandre}
\author[1]{Thalita F. Drumond}
\author[1]{Xavier Hinaut}
\author[1]{Thierry Vi\'eville}
\affil[1]{Mnemosyne team, INRIA Bordeaux}
\maketitle
\begin{abstract}\input{conclusion}\end{abstract}
\fi
\iffalse
\fi
\input{introduction}
\input{position}
\input{estimation}
\input{experimentation}
\section{Conclusion}\input{conclusion}
\appendix
\clearpage\input{generality}
\clearpage\input{backpropag}
\clearpage\input{application}
\clearpage\input{closedforms}
\clearpage\input{stochastic}
\clearpage{\scriptsize \bibliographystyle{plain} \bibliography{../bib/vthierry,../bib/from-keops,../bib/from-sophia}}
\tableofcontents
\iffalse
\fi

\end{document}

