\section*{Formalizing the recurrent weight estimation}

We implement the recurrent weight estimation as a variational problem, i.e. define weights as: \begin{equation}\label{eq-criterion} {\bf W} = \mbox{arg min}_{{\bf W} , {\bf x}} \; {\cal L}({\bf W} , {\bf x}) ,\end{equation}
writing:
\[ \begin{array}{clll} \multicolumn{3}{l}{{\cal L}({\bf W} , {\bf x}) \deq} \\
& \sum_{nt} & \rho_{nt}(x_n(t)) & \mbox{\small desired values} \\
-& \sum_{nt} & \varepsilon_{nt} \, (\hat{x}_n(t) - x_n(t)) & \mbox{\small network dynamic constraint} \\
+& {\cal R}({\bf W}) & & \mbox{\small regularization} \\
\end{array} \]
and represent the dynamic network recurrent equation via the notation of equation~(\ref{eq-recurrent}):
 \[ \left\{ \begin{array}{rcl}
\hat{x}_n(t) &\deq& \Phi_{n0t}\left(\cdots, x_{n'}(t'), \cdots, i_{m}(s), \cdots\right) \\
 &+& \sum_{d = 1}^{D_{n}} W_{nd} \, \Phi_{ndt}\left(\cdots, x_{n'}(t'), \cdots, i_{m}(s), \cdots\right)
\end{array} \right. \]
while $\varepsilon_{nt}$ are Lagrange multipliers.

Here $\rho_{nt}()$ is a cost-function (acting both as supervised and unsupervised variational term, as detailed in the next section) and ${\cal R}({\bf W})$ a regularization term. The cost function includes both the term attached to the data, i.e., the fact that output values have a desired values, and regularization. These ingredients are going to be used in the sequel to control approximate desired output, yield sparse estimation, reduce artifact influence, obtain activity orthogonality, etc.

Stating the estimation this way, leads us to a simplified form of the Pontryagin's minimum principle, well-known in control theory \cite{astrom:83}. The effective related solution is derived from the normal equations of the proposed criterion.

This formulation is not new and has been formalized, by, e.g. \cite{cun_theoretical_1988}. Here we restate it at a higher level of generality, with two new aspects: (i) making explicit the role of the Lagrange multiplier (also called adjoint state in this context) for hidden units and (ii) embedding this mechanism in more general learning schemes.

Applying standard derivations, the criterion gradient writes:
\[ \begin{array}{rcll}
\nabla_{\varepsilon_{nt}} \, {\cal L} &=& \hat{x}_n(t) - x_n(t) \\
&&\\
\nabla_{x_{n'}(t')} \, {\cal L} &=&  
-\varepsilon_{n't'} + \rho'_{n't'} + \sum_{nt, \begin{array}{c} t' < t \leq t' + R \\ or\; t' = t, n < n'\end{array}} \beta_{n't'}^{nt} \, \varepsilon_{nt} \\
&&\\
\nabla_{W_{nd}} \, {\cal L}  &=& \sum_{n'', W_{n''d} = W_{nd}} \sum_t \phi_{n''dt} \, \varepsilon_{n''t} + \nabla_{W_{nd}} \, {\cal R} \\
\end{array} \]
writing : {\small \[ \left\{ \begin{array}{rclcl}
\varepsilon_{nt} &\deq&  \varepsilon_{nt} + \rho'_{nt}\\
&\\
\rho'_{nt} &\deq&\rho'_{nt} \, (\hat{x}_n(t)) \\
&\\
\phi_{ndt} &\deq& \Phi_{ndt}\left(\cdots, x_{n'}(t'), \cdots, i_{m}(s), \cdots\right) &=& \nabla_{W_{nd}} \hat{x}_n(t)\\
&\\
\beta_{n't'}^{nt} &\deq& \frac{\partial \phi_{n0t}}{\partial x_{n'}(t')} + \sum_{d = 1}^{D_{n}} W_{nd} \, \frac{\partial \phi_{ndt}}{\partial x_{n'}(t')} &=& \nabla_{x_{n'}(t')} \hat{x}_n(t) \\
&\\
\end{array} \right. \]}

The sum $\sum_{nt, \begin{array}{c} t' < t \leq t' + R \;or\; t' = t, n < n'\end{array}}$ encounters for previous values and subsequent node values. This sum includes terms with $\beta_{n't'}^{nt} \neq 0$, i.e. terms for which there is a recurrent connection from the node of index $n$ at time $t$ onto the node of index $n'$ at time $t'$. We simply write $\sum_{nt}$ in the sequel, without any risk of ambiguity.

The sum $\sum_{n'', W_{n''d} = W_{nd}}$ encounters for weight sharing, i.e., the fact that weights from different units may be constrained to have the same value. We will simply write $\sum_{n''}$ in the sequel, without any risk of ambiguity.

Let us now review and discuss how we can implement such a minimization.

\subsection*{The minimization steps}

\subsubsection*{Forward simulation}

The equation $\nabla_{\varepsilon_{nt}} \, {\cal L} = 0$ yields $x_n(t) = \hat{x}_n(t)$, which simply corresponds to equation~(\ref{eq-recurrent}). Since $\hat{x}_n(t)$ depends on previous values at time $t' < t$, it provides a closed-form formula to evaluate $x_n(t)$ from the beginning to the end. This simply corresponds to the fact that the dynamic is simulated. This step depends on the weights $W_{nd}$ but not on the Lagrange multipliers $\varepsilon_{nt}$. At the end of the step the equality $\nabla_{\varepsilon_{nt}} \, {\cal L} = 0$ is obtained, and the criterion value itself does not depends on $\varepsilon$ since the constraints are verified. As a consequence, the criterion value ${\cal L}$ can be calculated during this step.

The forward simulation complexity corresponds to the network simulation and is of order $O(N D T)$ with a memory resources of $O(N T)$ since we must buffer the calculated output,  for subsequent calculations.

\subsubsection*{Backward tuning}

The equation $\nabla_{x_{n'}(t')} \, {\cal L}  = 0$ also provides a closed-form formula to evaluate $\varepsilon_{n't'}$ as a linear function of subsequent values $\varepsilon_{nt}, t > t'$, so that the calculation is to be done from the last time $t = T-1$ backward to the first time $t = 0$.

This is the key feature of such a variational approach, allowing backward tuning, i.e., take into account the fact that adjusting the system parameters for a node $n$ at time $t$ is interdependent with the state of subsequent computations.

If $\beta_{n't'}^{nt} = 0$, there is no dependency of ${x}_n(t)$ on ${x}_{n'}(t')$, and in the absence of recurrent connection $\varepsilon_{n't'} = 0$ and $\varepsilon_{n't'}$ only depends on the state cost function $\rho'_{nt}$.

As mentioned by \cite{cun_theoretical_1988}, $\beta_{n't'}^{nt}$ is nothing more than the first order approximation of the backward dynamics, technically the product of the weight matrix with the system Jacobian.

This backward computation is local to a given unit in the sense that only efferent units (i.e., units this unit is connected to) are involved in the computation of the related Lagrange parameter. This step depends on both weights and output values, and the equality $\nabla_{\varepsilon_{nt}} \, {\cal L} = 0$ is obtained at the end.

The backward tuning step has the same order of magnitude in terms of calculation $O(N D T)$  and memory resources of $O(N T)$ (in fact of $O(N R)$, because the obtained result may be immediately re-used to compute the weight adjustment gradient or 2nd order system). 

\paragraph{\em Parameter interpretation.}

We obtain, after some algebra:
\\\centerline{$\varepsilon_{n't'} = \rho'_{nt} + \sum_{nt} \beta_{n't'}^{nt} \, \varepsilon_{nt} = \sum_{nt} B_{n't'}^{nt} \, \rho'_{nt}$}\\ with finite summations and for some quantities $B_{n't'}^{nt}$ (not made explicit here) which are unary coefficient polynomial in $\beta_{n't'}^{nt}$. This made explicit the fact $\varepsilon_{n't'}$ is a linear function of subsequent errors, i.e., a backward tuning error.

If the unit has no recurrent connection, i.e. is a not a function of other units, then $\varepsilon_{n't'} = \rho'_{nt}$ is simply related to the cost function. In the least-square case (i.e. if $\rho_{nt} = \frac{1}{2} \, (x_{nt} - \bar{o}_{nt})^2$), then $\rho'_{nt} = x_{nt} - \bar{o}_{nt}$ is the output error.

It must be noted, that this method is quite different from back-propagation-though-time recurrent network estimation or other standard alternatives. If the calculation may be recognized as a kind of back-propagation, it is mathematically different.

\paragraph{\em Numerical stability.}

However, as reviewed in e.g., \cite{Hochreiter:1997} this back-propagation of tuning error, will suffer form the same curse than back-propagation of gradient: Either error explosion (if $|\beta_{n't'}^{nt}| > 1$), or error vanishing (if $|\beta_{n't'}^{nt}| < 1$). In our case, since all kernels are contracting we have the bound, writing $\beta_{max} \deq \max_{nt} |\beta_{n't'}^{nt}|$ :
\\\centerline{$0 \leq \left|\beta_{n't'}^{nt}\right| \leq \beta_{max} \leq 1 + \sum_d |W_{nd}|$}\\
without any thinner inequality in the general case. Based on this remark, the key idea of LSTM \cite{Hochreiter:1997} is to consider memory carousel as reviewed in the previous section to guaranty $\left|\beta_{n't'}^{nt}\right| \simeq 1$ and thus a stable back-propagation for at least some recurrent link. In our framework this means that it is the responsibility of the designer of the network architecture to consider nodes with such property.

Here, we are going to introduce another heuristic and {\em bias} the backward error in order to avoid both error explosion and vanishing. To this end, we define:
\begin{equation} \label{kappa}
\kappa_{n't'} = \rho'_{nt} + g_{\epsilon}\left(\sum_{nt} \beta_{n't'}^{nt} \, \kappa_{nt}, \beta_{max}\right),
\end{equation} considering a function $g_{\epsilon}(u, \beta_{max})$ yielding both amplification of small errors and saturation of larger error, while at the algorithm convergence we need to have $g(u) = u$ in order to yield an unbiased estimation. Here is a continuation parameter, with $\epsilon = 1$ at the minimization start, while $\epsilon \rightarrow 0$ with algorithm convergence. It is adjusted by the minimization algorithm, which looks for a convergence at $\epsilon = 1$ and then decreases $\epsilon$ while tracking the minimum during $\epsilon$ decrease.

A simple choice is to consider an exponential saturation. Taking into account the previous requirements, for $0 < \epsilon < 1$, writes:
\\\centerline{$g_{\epsilon}(u, \beta_{max}) \deq  sg(u) \, \left(1 + \beta_{max}\,\frac{1-\epsilon}{\epsilon}\right) \, \left(1 - e^{-\frac{\epsilon}{\beta_{max}} \, |u|}\right)$}\\
and it is obvious to verify that:
\\\centerline{$\begin{array}{rcll} g_{\epsilon}(u, \beta_{max}) 
 &=& u + O(\epsilon) & \mbox{convergence to identity when $\epsilon \rightarrow 0$} \\
 &=& \frac{u}{\beta_{max}} + O(u^2) + O(1 - \epsilon) & \mbox{amplification around $\epsilon = 1$} \\
 &<& \frac{1}{\epsilon} & \mbox{saturation for non negligible $\epsilon$} \\
 &\leq& \left((1 - \epsilon) + \frac{\epsilon}{\beta_{max}}\right) \, u & \mbox{concave profile} \\
\end{array}$}\\
The convergence to identify is uniform for bounded values of $u$ and the amplification leads to $\left|\frac{\partial g(\dot)}{\partial \kappa_{nt}}\right| \leq 1$ but as closed as possible to 1, avoiding any propagation explosion, with at least some non negligible backward tuning. With the decrease of $\epsilon$ we may and must relax this constraint, but can reasonably expect the backward tuning error to decrease with the criterion convergence, the related quantities to remain bounded.

\paragraph{\em Real-time aspects.}

Such a formulation is definitely not ``real-time'', since we ``go back in time''. It is however, the only solution for hidden layers to be tuned, since the output is a function of hidden activity in the past. 

However, in a real-time paradigm, it must be noted that each computation is also local in {\em time}: It only depends on values in a ``near future'' within a time range equal to the system time range. In other words, at a given time we obtain the value with a lag equal to system time-range. It is an interesting perspective of this work to explore if, in a rather stationary context, it may provide numerically relevant values for on-the-fly backward tuning.

\subsubsection*{The 1st order unit weight adjustment}

The calculation of $\nabla_{W_{nd}}\, {\cal L} $ yields a Hebbian weight adaptation rule (as the sum of products between an output unit error term $\varepsilon_{nt}$ (combining the supervised error and the backward tuning multiplier) and an input quantity $\phi_{ndt}$. This rule applies to both output unit of index $n < N_0$ with a desired output and hidden units of index $N_0 \leq n$ that indirectly adapt their behavior to optimize the output, via the backward tuning values. The gradient calculation is local to a given unit and average over time, through another $O(NDT)$ computation.

This leads to a local 1st order adjustment of the weights, i.e. it provides the direction for the weight variation, not its magnitude.

To numerically adjust this magnitude we have to perform a 1D minimization, i.e., $\mbox{min}_{\upsilon^k} \; {\cal L}({\bf W}(\upsilon^k) , \tilde{\bf x})$, with:
\\\centerline{$W_{nd}(\upsilon^k) = \tilde{W}_{nd} - \upsilon^k \, \nabla_{\tilde{W}_{nd}} \, {\cal L}, \mbox{ with } 0 < \upsilon^k$} \\
where $\tilde{\bf W}$ and $\tilde{\bf x}$ correspond to the previous estimation of the weights and activity. This minimization can be local to each unit. The key point is that we do not know any reasonable upper-bound for $\upsilon^k$. As a consequence standard golden section search or Brent-Dekker methods do not directly apply, and we have to numerically find such an upper-bound first, and it appeared that which requires additional calculation steps. It is also known that it is inefficient to precisely find the minimum since the gradient value varies with $W_{nd}$ so that the result is anyway biased. Taking all this into account, we obtain the 1st order unit weight adjustment heuristic:\begin{quotation}{\small 
\noindent -0- Starts with the last known value of $\upsilon^k$ or very small value (here $10^{-6}$).
\\-1- Computes ${\cal L}(W_{nd}(\upsilon^k))$.
\\\hspace{0.5cm} -2.1- If it decreases, register this better value, and set $\upsilon^k = 2\, \upsilon^k$ for the next step, 
\\\hspace{0.5cm} -2.2- else set $\upsilon^k = \upsilon^k / 3$.
\\-3- Repeat -1- unless steps -2.2- leads to a negligible value of $\upsilon^k$.
}\end{quotation}
Each step requires a simulation to compute ${\cal L}$.

It is easy to verify that this tiny heuristic converges and provides an approximation of the minimum. It also provides an upper-bound for usual minimum search methods, but it has be numerically observed, at the global minimization level, that adding such search methods is useless (it slows-down the minimization with only a negligible gain in precision, if any).

As far as backward tuning stability is concerned the value of $\varepsilon_{nt}$ is replaced by its approximation $\kappa_{nt}$.

The 1st order unit weight adjustment could either be done globally at the whole node set level, or locally for each unit, the criterion itself being decomposable on each unit. In this latter case, the global convergence is guaranty only with infinitesimal gradient steps.

\subsubsection*{The 2nd order unit weight adjustment}

Due to the simplicity of the approach, we can write a 2nd order weight adjustment:
\[
\nabla_{W_{nd}} \, {\cal L} = 0 \Rightarrow \sum_{n''} b_{n'',\,d} = \sum_{n''} \sum_{d'=1}^{D_{n}} A_{n'',\,d\;d'} \, W_{nd'}
\] writing, for some $\kappa_{nt}$: \[ \left\{ \begin{array}{rcl}
b_{n,\,d} &\deq& \sum_t \phi_{ndt} \, \left( \varepsilon_{nt}  + \kappa_{nt} \, \left(\hat{x}_n(t) - \phi_{n0t} \right) \right)  + \nabla_{W_{nd}} \, {\cal R}(\tilde{\bf W}),  \\
&\\
A_{n,\,d\;d'} &\deq& \sum_t \kappa_{nt} \, \phi_{ndt} \, \phi_{nd't}.  \\
\end{array} \right. \]
This allows to obtain a new weight value $\tilde{\bf W}$ solving a linear system of equation for each unit, where $\hat{x}_n(t)$ is calculated taking the previous or initial weight value $\hat{\bf W}$ into account.

Here indeed, the value of $\kappa_{nt}$ corresponds to the backward tuning stability proposed heuristic.

The chosen form is related to the 2nd order Hessian of the criterion\footnote{
\label{criterion-hessian} The criterion Hessian, omitting the regularization term, writes: \\ \centerline{$\left.\begin{array}{rcl}
\nabla_{W_{nd} W_{n'd'}} \, {\cal L} &=& \delta_{n=n'} \, \sum_t \, \rho''_{nt} \, \phi_{ndt} \, \phi_{nd't} 
\end{array}\right.$}\\
\\ Here the notation $\delta_{\cal P}$ stands for $1$ is the property ${\cal P}$ is true and $0$ otherwise.
\\ A step further, for the sake of completeness we also make explicit (writing $\beta_{nt}^{nt} \deq -1$ ):
\\ \centerline{$\left\{\begin{array}{rcl}
\nabla_{\varepsilon_{nt} \varepsilon_{n't'}} \, {\cal L} &=& 0 \\
&&\\
\nabla_{x_{n'}(t') \varepsilon_{nt}} \, {\cal L} &=& \beta_{n't'}^{nt} \\
&&\\
\nabla_{W_{nd} \varepsilon_{n't'}} \, {\cal L} &=& \delta_{n=n'} \, \phi_{ndt'} \\
&&\\
\nabla_{x_{n'}(t') x_{n''}(t'')} \, {\cal L} &=&  \sum_{nt} \rho''_{nt} \, \beta_{n't'}^{nt} \, \beta_{n''t''}^{nt}
 \\&+& \left[ \frac{\partial^2 \phi_{n0t}}{\partial x_{n'}(t') x_{n''}(t'')} + \sum_{d = 1}^{D_{n}} W_{nd} \, \frac{\partial^2 \phi_{ndt}}{\partial x_{n'}(t') x_{n''}(t'')} \right] \, \varepsilon_{nt}, \\
&&\\
\nabla_{W_{nd} x_{n'}(t')} \, {\cal L} &=&  \sum_{t} \rho''_{nt} \, \beta_{n't'}^{nt} \, \phi_{ndt}
 + \frac{\partial \phi_{ndt}}{\partial x_{n'}(t')} \, \varepsilon_{nt}, \\
\end{array}\right.$.}} and generalizes the readout least-square estimation of weights\footnote{
For a simple least-square criterion of the form:
\\ \centerline{${\cal L} = \sum_{nt} \frac{\kappa_{nt}}{2} \, (\hat{x}_n(t) - \bar{o}_n(t))^2$} \\
where $\kappa_{nt} \in \{0, 1\}$ depending on the fact that the desired output $\bar{o}_n(t)$ is defined or not, it is straight-forward to verify the proposed 2nd order weight adjustment holds, and reduces to an exact linear system of equation, in the absence of recurrent links of the given unit, since $\phi_{ndt}$ is only function of the input. Otherwise, we only $\phi_{ndt}$ is also a function of both the network unknown output and hidden node values. For output node value the $\bar{o}_n(t)$ desired value could be enforced, limiting recurrent perturbation and yielding $\phi_{ndt}$ values closed to the ideal value, which is interesting in reverse-engineering estimation, i.e. when an exact solution is expected \cite{rostro-gonzalez-cessac-etal:10}, whereas a bias in the estimation is otherwise introduced.} in a neural network.

The weight adjustment is local to each unit, providing a true distributed mechanism. This corresponds to a 2nd order minimization scheme. Each step requires $O(N (D T + D^3)$ operation, solving a linear system of equations. The implemented method is a Cholesky decomposition with a fallback onto a singular-value-decomposition if the ${\bf A}_n$ is not strictly positive.

Taking all this into account, we obtain the following 2nd order unit weight adjustment heuristic:\begin{quotation}{\small 
\noindent -0- Calculates $\tilde{\bf W}$ and starts with $\upsilon^k = 1$.
\\-1- Computes ${\cal L}(\upsilon^k \tilde{\bf W} + (1 - \upsilon^k) \, \hat{\bf W})$.
\\\hspace{0.5cm} -2.1- If it decreases, register this better value, 
\\\hspace{0.5cm} -2.2- else set $\upsilon^k = \upsilon^k / 2$.
\\-3- Repeat -1- unless steps -2.2- leads to a negligible value of $\upsilon^k$.
}\end{quotation}
In words we look for a weight value between both previous and new values that decreases the criterion. Each step requires a simulation to compute ${\cal L}$.

\subsubsection*{The complete weight adjustment}

Collecting previous steps the final iterative weight adjustment writes\begin{quotation}{\small 
\noindent -1- Performs a forward simulation and a backward tuning, calculating the 1st order gradient and 2nd order elementsduring the backward estimation.
\\\hspace{0.5cm} -2.a- Attempt to perform a 2nd order weight adjustment.
\\\hspace{0.5cm} -2.b- If it fails, attempt to perform a 1st order weight adjustment.
\\-3- Repeat -1- unless steps -2.b- fails.
}\end{quotation}
This weight adjustment has to be performed by continuation using the modified numerically stable backward tuning calculation.

The present proposal stands on the fact that we have been able to derive a local 2nd order adjustment based on renormalized backward tuning. The 1st order fall-back method could indeed have been enhanced using either the so called momentum gradient mechanism (based on a temporal averaging of the gradient values), or the partial conjoint gradient methods (taking into account several subsequent gradient directions in order to infer an approximate 2nd order mimimization method. We also propose here an alternative to 2nd order adjustment methods such as \cite{martens_learning_2016} or other methods reviewed in \cite{Goodfellow2016Deep}.

